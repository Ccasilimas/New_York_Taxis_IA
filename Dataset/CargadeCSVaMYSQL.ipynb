{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script de Importación de Trafico de NYC a MySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import mysql.connector\n",
    "from datetime import datetime\n",
    "\n",
    "# Cargar las variables de entorno desde el archivo .env\n",
    "load_dotenv()\n",
    "\n",
    "# Obtener las variables de entorno\n",
    "db_host = os.getenv('DB_HOST')\n",
    "db_user = os.getenv('DB_USER')\n",
    "db_password = os.getenv('DB_PASSWORD')\n",
    "db_name = os.getenv('DB_NAME')\n",
    "db_port = os.getenv('DB_PORT')\n",
    "csv_file_path = 'Automated_Traffic_Volume_Counts.csv'  # Asegúrate de actualizar esto con tu archivo CSV correcto\n",
    "\n",
    "# Verificar si las variables de entorno se cargaron correctamente\n",
    "if not all([db_host, db_user, db_password, db_name, db_port]):\n",
    "    raise ValueError(\"No se pudieron cargar todas las variables de entorno. Por favor verifica el archivo .env.\")\n",
    "\n",
    "# Cargar el archivo CSV en un DataFrame de pandas\n",
    "try:\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "except FileNotFoundError as e:\n",
    "    raise FileNotFoundError(f\"El archivo {csv_file_path} no se encuentra. Por favor verifica la ruta.\")\n",
    "\n",
    "# Filtrar los datos desde enero 2023 hasta agosto 2024\n",
    "start_date = datetime(2023, 1, 1)\n",
    "end_date = datetime(2024, 8, 31)\n",
    "\n",
    "# Ensure the columns are in the correct format\n",
    "df['Yr'] = df['Yr'].astype(int)\n",
    "df['M'] = df['M'].astype(int)\n",
    "df['D'] = df['D'].astype(int)\n",
    "df['HH'] = df['HH'].astype(int)\n",
    "df['MM'] = df['MM'].astype(int)\n",
    "\n",
    "# Create a datetime column\n",
    "df['datetime'] = pd.to_datetime(df[['Yr', 'M', 'D', 'HH', 'MM']].rename(columns={'Yr': 'year', 'M': 'month', 'D': 'day', 'HH': 'hour', 'MM': 'minute'}))\n",
    "\n",
    "df_filtered = df[(df['datetime'] >= start_date) & (df['datetime'] <= end_date)].copy()\n",
    "\n",
    "# Seleccionar solo las columnas necesarias para el análisis\n",
    "columns_to_keep = ['RequestID', 'Boro', 'Yr', 'M', 'D', 'HH', 'MM', 'Vol', 'SegmentID', 'WktGeom', 'street', 'Direction']\n",
    "df_filtered = df_filtered[columns_to_keep]\n",
    "\n",
    "# Conectar a la base de datos MySQL\n",
    "try:\n",
    "    db_connection = mysql.connector.connect(\n",
    "        host=db_host,\n",
    "        user=db_user,\n",
    "        password=db_password,\n",
    "        database=db_name,\n",
    "        port=int(db_port)\n",
    "    )\n",
    "except mysql.connector.Error as err:\n",
    "    raise mysql.connector.Error(f\"Error al conectar a la base de datos: {err}\")\n",
    "\n",
    "cursor = db_connection.cursor()\n",
    "\n",
    "# Insertar los datos filtrados del DataFrame en bloques de 100,000 filas\n",
    "chunk_size = 100000\n",
    "rows_inserted = 0\n",
    "for start in range(0, len(df_filtered), chunk_size):\n",
    "    end = start + chunk_size\n",
    "    chunk = df_filtered.iloc[start:end]\n",
    "\n",
    "    insert_query = \"\"\"\n",
    "    INSERT INTO trafico (RequestID, Boro, Yr, M, D, HH, MM, Vol, SegmentID, WktGeom, Direction)\n",
    "    VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "    \"\"\"\n",
    "    data = [(\n",
    "        row['RequestID'], row['Boro'], row['Yr'], row['M'], row['D'], row['HH'], row['MM'], row['Vol'],\n",
    "        row['SegmentID'], row['WktGeom'], row['Direction']\n",
    "    ) for _, row in chunk.iterrows()]\n",
    "\n",
    "    cursor.executemany(insert_query, data)\n",
    "    rows_inserted += len(data)\n",
    "    \n",
    "    # Imprimir el progreso solo cada 10 bloques\n",
    "    if start // chunk_size % 10 == 0:\n",
    "        print(f\"Se han insertado {len(data)} filas en este bloque. Total filas insertadas: {rows_inserted}\")\n",
    "\n",
    "# Confirmar los cambios y cerrar la conexión\n",
    "db_connection.commit()\n",
    "cursor.close()\n",
    "db_connection.close()\n",
    "\n",
    "# Mostrar el número total de filas insertadas\n",
    "print(f\"Se han insertado un total de {rows_inserted} filas en la base de datos.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script de Importación de Temperaturas Promedio de NYC a MySQL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import mysql.connector\n",
    "\n",
    "# Cargar las variables de entorno desde el archivo .env\n",
    "load_dotenv()\n",
    "\n",
    "# Obtener las variables de entorno\n",
    "db_host = os.getenv('DB_HOST')\n",
    "db_user = os.getenv('DB_USER')\n",
    "db_password = os.getenv('DB_PASSWORD')\n",
    "db_name = os.getenv('DB_NAME')\n",
    "db_port = os.getenv('DB_PORT')\n",
    "csv_file_path = 'temperaturas_promedio_nyc_mensual.csv'  # Asegúrate de actualizar esto con tu archivo CSV correcto\n",
    "\n",
    "# Verificar si las variables de entorno se cargaron correctamente\n",
    "if not all([db_host, db_user, db_password, db_name, db_port]):\n",
    "    raise ValueError(\"No se pudieron cargar todas las variables de entorno. Por favor verifica el archivo .env.\")\n",
    "\n",
    "# Cargar el archivo CSV en un DataFrame de pandas\n",
    "try:\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "except FileNotFoundError as e:\n",
    "    raise FileNotFoundError(f\"El archivo {csv_file_path} no se encuentra. Por favor verifica la ruta.\")\n",
    "\n",
    "# Conectar a la base de datos MySQL\n",
    "try:\n",
    "    db_connection = mysql.connector.connect(\n",
    "        host=db_host,\n",
    "        user=db_user,\n",
    "        password=db_password,\n",
    "        database=db_name,\n",
    "        port=int(db_port)\n",
    "    )\n",
    "except mysql.connector.Error as err:\n",
    "    raise mysql.connector.Error(f\"Error al conectar a la base de datos: {err}\")\n",
    "\n",
    "cursor = db_connection.cursor()\n",
    "\n",
    "# Insertar los datos del DataFrame en la tabla MySQL\n",
    "insert_query = \"\"\"\n",
    "INSERT INTO temperaturas (Mes, Manhattan, Brooklyn, Queens, The_Bronx, Staten_Island)\n",
    "VALUES (%s, %s, %s, %s, %s, %s)\n",
    "\"\"\"\n",
    "data = [tuple(row) for row in df.values]\n",
    "\n",
    "cursor.executemany(insert_query, data)\n",
    "\n",
    "# Confirmar los cambios y cerrar la conexión\n",
    "db_connection.commit()\n",
    "cursor.close()\n",
    "db_connection.close()\n",
    "\n",
    "# Mostrar el número de filas insertadas\n",
    "print(f\"Se han insertado {len(data)} filas en la base de datos.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script de Importación de Taxi_Zones de NYC a MySQL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sqlalchemy import create_engine, text\n",
    "from dotenv import load_dotenv\n",
    "from shapely import wkt\n",
    "\n",
    "# Cargar las variables de entorno desde el archivo .env\n",
    "load_dotenv()\n",
    "\n",
    "# Configuración de la conexión a la base de datos con credenciales desde variables de entorno\n",
    "DB_CONFIG = {\n",
    "    'host': os.getenv(\"DB_HOST\"),\n",
    "    'port': int(os.getenv(\"DB_PORT\", 3306)),\n",
    "    'user': os.getenv(\"DB_USER\"),\n",
    "    'password': os.getenv(\"DB_PASSWORD\"),\n",
    "    'database': os.getenv(\"DB_NAME\")\n",
    "}\n",
    "\n",
    "def create_and_populate_taxi_zones_table(csv_file_path):\n",
    "    try:\n",
    "        # Leer el archivo CSV en un DataFrame de pandas\n",
    "        df = pd.read_csv(csv_file_path)\n",
    "\n",
    "        # Convertir la columna 'the_geom' a objetos geométricos utilizando shapely\n",
    "        df['geometry'] = df['the_geom'].apply(lambda x: wkt.loads(x) if pd.notnull(x) else None)\n",
    "        \n",
    "        # Eliminar las columnas adicionales que no están en la tabla 'taxi_zones'\n",
    "        df = df[['LocationID', 'borough', 'zone', 'the_geom']]\n",
    "\n",
    "        # Eliminar filas duplicadas basadas en 'LocationID'\n",
    "        df = df.drop_duplicates(subset='LocationID')\n",
    "\n",
    "        # Crear el motor de SQLAlchemy para las operaciones de base de datos\n",
    "        engine = create_engine(\n",
    "            f\"mysql+pymysql://{DB_CONFIG['user']}:{DB_CONFIG['password']}@\"\n",
    "            f\"{DB_CONFIG['host']}:{DB_CONFIG['port']}/{DB_CONFIG['database']}\"\n",
    "        )\n",
    "\n",
    "        # Definir el esquema de la tabla utilizando text\n",
    "        create_table_query = text(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS taxi_zones (\n",
    "            LocationID INT PRIMARY KEY,\n",
    "            Borough VARCHAR(50) NOT NULL,\n",
    "            Zone VARCHAR(100) NOT NULL,\n",
    "            the_geom TEXT NOT NULL\n",
    "        );\n",
    "        \"\"\")\n",
    "\n",
    "        # Ejecutar la creación de la tabla usando el método correcto\n",
    "        with engine.connect() as connection:\n",
    "            connection.execute(create_table_query)\n",
    "\n",
    "        # Guardar los datos transformados en la base de datos\n",
    "        df.to_sql('taxi_zones', engine, if_exists='append', index=False)\n",
    "\n",
    "        print(\"🚀 Tabla taxi_zones creada y datos insertados exitosamente\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error durante la creación o inserción de datos en la tabla taxi_zones: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# Ruta al archivo CSV\n",
    "csv_file_path = 'taxi_zones.csv'\n",
    "\n",
    "# Ejecutar la función para crear la tabla y poblarla con datos\n",
    "if __name__ == \"__main__\":\n",
    "    create_and_populate_taxi_zones_table(csv_file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consultas en MYSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# 📝 Configuración del logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, \n",
    "    format='%(asctime)s - %(levelname)s: %(message)s',\n",
    "    filename='database_query.log'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class DatabaseManager:\n",
    "    \"\"\"🗄️ Administrador de conexiones y consultas a la base de datos\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # 🔑 Cargar variables de entorno\n",
    "        load_dotenv()\n",
    "        \n",
    "        # 🔒 Obtener credenciales de la base de datos\n",
    "        self.db_host = os.getenv('DB_HOST')\n",
    "        self.db_user = os.getenv('DB_USER')\n",
    "        self.db_password = os.getenv('DB_PASSWORD')\n",
    "        self.db_name = os.getenv('DB_NAME')\n",
    "\n",
    "    def _create_connection(self):\n",
    "        \"\"\"🔌 Crear una conexión a la base de datos\"\"\"\n",
    "        try:\n",
    "            conn = mysql.connector.connect(\n",
    "                host=self.db_host,\n",
    "                user=self.db_user,\n",
    "                password=self.db_password,\n",
    "                database=self.db_name\n",
    "            )\n",
    "            logger.info(\"🟢 Conexión establecida exitosamente\")\n",
    "            return conn\n",
    "        except mysql.connector.Error as err:\n",
    "            logger.error(f\"🔴 Error de conexión: {err}\")\n",
    "            raise\n",
    "\n",
    "    def count_records(self):\n",
    "        \"\"\"📊 Ejecutar un COUNT(1) en la tabla taxi_fhv_data\"\"\"\n",
    "        conn = None\n",
    "        cursor = None\n",
    "        try:\n",
    "            conn = self._create_connection()\n",
    "            cursor = conn.cursor()\n",
    "            \n",
    "            # 🔍 Ejecutar la consulta COUNT(1)\n",
    "            query = \"SELECT COUNT(1) FROM taxi_fhv_data\"\n",
    "            cursor.execute(query)\n",
    "            count = cursor.fetchone()[0]\n",
    "            logger.info(f\"📈 Total de registros: {count}\")\n",
    "            print(f\"📈 Total de registros: {count}\")\n",
    "            return count\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"❌ Error en la consulta: {e}\")\n",
    "            print(f\"❌ Error en la consulta: {e}\")\n",
    "            raise\n",
    "            \n",
    "        finally:\n",
    "            # 🧹 Limpieza de recursos\n",
    "            if cursor:\n",
    "                cursor.close()\n",
    "            if conn:\n",
    "                conn.close()\n",
    "                logger.info(\"🔌 Conexión cerrada\")\n",
    "            \n",
    "            # ✅ Registro de finalización\n",
    "            print(\"✅ Proceso de consulta completado\")\n",
    "            logger.info(\"✅ Proceso de consulta completado\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"🎯 Función principal de ejecución\"\"\"\n",
    "    db_manager = DatabaseManager()\n",
    "    \n",
    "    try:\n",
    "        record_count = db_manager.count_records()\n",
    "        print(f\"🎉 Conteo exitoso: {record_count} registros\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ El proceso falló: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nueva tabla para machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sqlalchemy import create_engine\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "\n",
    "# 🌟 Cargar variables de entorno\n",
    "load_dotenv()\n",
    "\n",
    "# 🔐 Configuración de conexión a la base de datos\n",
    "DB_CONFIG = {\n",
    "    'host': os.getenv(\"DB_HOST\"),\n",
    "    'port': int(os.getenv(\"DB_PORT\", 3306)),\n",
    "    'user': os.getenv(\"DB_USER\"),\n",
    "    'password': os.getenv(\"DB_PASSWORD\"),\n",
    "    'database': os.getenv(\"DB_NAME\")\n",
    "}\n",
    "\n",
    "def fetch_and_process_batch(offset, batch_size):\n",
    "    try:\n",
    "        engine = create_engine(\n",
    "            f\"mysql+pymysql://{DB_CONFIG['user']}:{DB_CONFIG['password']}@{DB_CONFIG['host']}:{DB_CONFIG['port']}/{DB_CONFIG['database']}\"\n",
    "        )\n",
    "        \n",
    "        query = f\"\"\"\n",
    "        SELECT \n",
    "            DATE(t.Pickup_datetime) AS pickup_date,\n",
    "            t.PULocationID,\n",
    "            z.Borough AS pickup_borough,\n",
    "            DAY(t.Pickup_datetime) AS pickup_day,\n",
    "            HOUR(t.Pickup_datetime) AS pickup_hour,\n",
    "            COUNT(*) AS trip_count\n",
    "        FROM \n",
    "            taxi_fhv_data t\n",
    "        JOIN \n",
    "            taxi_zones z ON t.PULocationID = z.LocationID\n",
    "        WHERE \n",
    "            t.Pickup_datetime >= DATE_SUB(CURDATE(), INTERVAL 44 MONTH)\n",
    "        GROUP BY\n",
    "            pickup_date, t.PULocationID, pickup_borough, pickup_day, pickup_hour\n",
    "        LIMIT {batch_size} OFFSET {offset}\n",
    "        \"\"\"\n",
    "        \n",
    "        df = pd.read_sql(query, engine)\n",
    "        \n",
    "        df.to_sql('enriched_taxi_data',\n",
    "                  engine,\n",
    "                  if_exists='append',\n",
    "                  index=False,\n",
    "                  chunksize=25000)\n",
    "        \n",
    "        return len(df)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error en el procesamiento del lote: {e}\")\n",
    "        return 0\n",
    "\n",
    "def populate_enriched_taxi_data_table(batch_size=25000):\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        engine = create_engine(\n",
    "            f\"mysql+pymysql://{DB_CONFIG['user']}:{DB_CONFIG['password']}@{DB_CONFIG['host']}:{DB_CONFIG['port']}/{DB_CONFIG['database']}\"\n",
    "        )\n",
    "        with engine.connect() as connection:\n",
    "            count_query = \"\"\"\n",
    "            SELECT COUNT(*) as total_rows \n",
    "            FROM taxi_fhv_data \n",
    "            WHERE Pickup_datetime >= DATE_SUB(CURDATE(), INTERVAL 44 MONTH)\n",
    "            \"\"\"\n",
    "            total_rows = pd.read_sql(count_query, connection)['total_rows'][0]\n",
    "\n",
    "        offsets = range(0, total_rows, batch_size)\n",
    "        processed_rows = 0\n",
    "        \n",
    "        for offset in offsets:\n",
    "            batch_start_time = time.time()\n",
    "            processed = fetch_and_process_batch(offset, batch_size)\n",
    "            processed_rows += processed\n",
    "\n",
    "            # Calcular tiempo de procesamiento del lote y estimar el tiempo restante\n",
    "            batch_time = time.time() - batch_start_time\n",
    "            remaining_batches = (total_rows - processed_rows) // batch_size\n",
    "            estimated_remaining = batch_time * remaining_batches\n",
    "\n",
    "            print(f\"🚦 Processed batch: {processed_rows}/{total_rows} rows \"\n",
    "                  f\"({processed_rows/total_rows*100:.2f}%) \"\n",
    "                  f\"| Last batch time: {batch_time:.2f}s \"\n",
    "                  f\"| Est. remaining: {estimated_remaining/60:.2f} mins\")\n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "        print(f\"✅ Data enrichment completed successfully\")\n",
    "        print(f\"⏱️ Total processing time: {total_time/60:.2f} minutes\")\n",
    "        print(f\"📊 Total rows processed: {processed_rows}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error during data enrichment: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# 🏁 Ejecutar el proceso de enriquecimiento\n",
    "if __name__ == \"__main__\":\n",
    "    populate_enriched_taxi_data_table()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# 🌟 Cargar variables de entorno\n",
    "load_dotenv()\n",
    "\n",
    "# 🔐 Configuración de la conexión a la base de datos\n",
    "DB_CONFIG = {\n",
    "    'host': os.getenv(\"DB_HOST\"),\n",
    "    'port': int(os.getenv(\"DB_PORT\", 3306)),\n",
    "    'user': os.getenv(\"DB_USER\"),\n",
    "    'password': os.getenv(\"DB_PASSWORD\"),\n",
    "    'database': os.getenv(\"DB_NAME\")\n",
    "}\n",
    "\n",
    "# Crear el motor de SQLAlchemy\n",
    "engine = create_engine(\n",
    "    f\"mysql+pymysql://{DB_CONFIG['user']}:{DB_CONFIG['password']}@{DB_CONFIG['host']}:{DB_CONFIG['port']}/{DB_CONFIG['database']}\"\n",
    ")\n",
    "\n",
    "# Leer los datos desde la tabla enriched_taxi_data\n",
    "query = \"SELECT * FROM enriched_taxi_data\"\n",
    "df = pd.read_sql(query, engine)\n",
    "\n",
    "# Seleccionar las características y la variable objetivo\n",
    "X = df[['PULocationID', 'pickup_borough', 'pickup_day', 'pickup_hour']]\n",
    "y = df['trip_count']\n",
    "\n",
    "# Convertir variable categórica 'pickup_borough' en variables dummy\n",
    "X = pd.get_dummies(X, columns=['pickup_borough'], drop_first=True)\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Crear y entrenar el modelo\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Realizar predicciones sobre el conjunto de prueba\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluar el rendimiento del modelo\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"MAE: {mae:.2f}\")\n",
    "print(f\"R²: {r2:.2f}\")\n",
    "\n",
    "# Función para predecir la demanda dada una zona, día y hora\n",
    "def predict_demand(pulocationid, pickup_day, pickup_hour, pickup_borough):\n",
    "    # Crear un DataFrame con las mismas columnas que el DataFrame original utilizado para entrenar el modelo\n",
    "    input_data = pd.DataFrame({\n",
    "        'PULocationID': [pulocationid],\n",
    "        'pickup_day': [pickup_day],\n",
    "        'pickup_hour': [pickup_hour]\n",
    "    })\n",
    "\n",
    "    # Añadir columnas dummy para 'pickup_borough'\n",
    "    borough_columns = [col for col in X.columns if col.startswith('pickup_borough_')]\n",
    "    for col in borough_columns:\n",
    "        input_data[col] = 0\n",
    "    \n",
    "    # Establecer el valor adecuado de la columna dummy correspondiente al 'pickup_borough'\n",
    "    if f'pickup_borough_{pickup_borough}' in input_data.columns:\n",
    "        input_data[f'pickup_borough_{pickup_borough}'] = 1\n",
    "    \n",
    "    prediction = model.predict(input_data)\n",
    "    return prediction[0]\n",
    "\n",
    "# Ejemplo de uso de la función predict_demand\n",
    "predicted_demand = predict_demand(11, 11, 13, 'Manhattan')\n",
    "print(f\"Predicted Demand: {predicted_demand:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crea Tabla para Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sqlalchemy import create_engine\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "\n",
    "# 🌟 Cargar variables de entorno\n",
    "load_dotenv()\n",
    "\n",
    "# 🔐 Configuración de conexión a la base de datos\n",
    "DB_CONFIG = {\n",
    "    'host': os.getenv(\"DB_HOST\"),\n",
    "    'port': int(os.getenv(\"DB_PORT\", 3306)),\n",
    "    'user': os.getenv(\"DB_USER\"),\n",
    "    'password': os.getenv(\"DB_PASSWORD\"),\n",
    "    'database': os.getenv(\"DB_NAME\")\n",
    "}\n",
    "\n",
    "def fetch_and_process_batch(offset, batch_size):\n",
    "    try:\n",
    "        engine = create_engine(\n",
    "            f\"mysql+pymysql://{DB_CONFIG['user']}:{DB_CONFIG['password']}@{DB_CONFIG['host']}:{DB_CONFIG['port']}/{DB_CONFIG['database']}\"\n",
    "        )\n",
    "        \n",
    "        query = f\"\"\"\n",
    "        SELECT \n",
    "            DATE(t.Pickup_datetime) AS pickup_date,\n",
    "            t.PULocationID,\n",
    "            DAYOFWEEK(t.Pickup_datetime) AS pickup_weekday,\n",
    "            HOUR(t.Pickup_datetime) AS pickup_hour,\n",
    "            COUNT(*) AS trip_count\n",
    "        FROM \n",
    "            taxi_fhv_data t\n",
    "        JOIN \n",
    "            taxi_zones z ON t.PULocationID = z.LocationID\n",
    "        WHERE \n",
    "            t.Pickup_datetime >= DATE_SUB(CURDATE(), INTERVAL 44 MONTH)\n",
    "        GROUP BY\n",
    "            pickup_date, t.PULocationID, pickup_weekday, pickup_hour\n",
    "        LIMIT {batch_size} OFFSET {offset}\n",
    "        \"\"\"\n",
    "        \n",
    "        df = pd.read_sql(query, engine)\n",
    "        \n",
    "        df.to_sql('enriched_taxi_data2',\n",
    "                  engine,\n",
    "                  if_exists='append',\n",
    "                  index=False,\n",
    "                  chunksize=25000)\n",
    "        \n",
    "        return len(df)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error en el procesamiento del lote: {e}\")\n",
    "        return 0\n",
    "\n",
    "def populate_enriched_taxi_data_table(batch_size=25000):\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        engine = create_engine(\n",
    "            f\"mysql+pymysql://{DB_CONFIG['user']}:{DB_CONFIG['password']}@{DB_CONFIG['host']}:{DB_CONFIG['port']}/{DB_CONFIG['database']}\"\n",
    "        )\n",
    "        with engine.connect() as connection:\n",
    "            count_query = \"\"\"\n",
    "            SELECT COUNT(*) as total_rows \n",
    "            FROM taxi_fhv_data \n",
    "            WHERE Pickup_datetime >= DATE_SUB(CURDATE(), INTERVAL 44 MONTH)\n",
    "            \"\"\"\n",
    "            total_rows = pd.read_sql(count_query, connection)['total_rows'][0]\n",
    "\n",
    "        offsets = range(0, total_rows, batch_size)\n",
    "        processed_rows = 0\n",
    "        \n",
    "        for offset in offsets:\n",
    "            batch_start_time = time.time()\n",
    "            processed = fetch_and_process_batch(offset, batch_size)\n",
    "            processed_rows += processed\n",
    "\n",
    "            # Calcular tiempo de procesamiento del lote y estimar el tiempo restante\n",
    "            batch_time = time.time() - batch_start_time\n",
    "            remaining_batches = (total_rows - processed_rows) // batch_size\n",
    "            estimated_remaining = batch_time * remaining_batches\n",
    "\n",
    "            print(f\"🚦 Processed batch: {processed_rows}/{total_rows} rows \"\n",
    "                  f\"({processed_rows/total_rows*100:.2f}%) \"\n",
    "                  f\"| Last batch time: {batch_time:.2f}s \"\n",
    "                  f\"| Est. remaining: {estimated_remaining/60:.2f} mins\")\n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "        print(f\"✅ Data enrichment completed successfully\")\n",
    "        print(f\"⏱️ Total processing time: {total_time/60:.2f} minutes\")\n",
    "        print(f\"📊 Total rows processed: {processed_rows}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error during data enrichment: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# 🏁 Ejecutar el proceso de enriquecimiento\n",
    "if __name__ == \"__main__\":\n",
    "    populate_enriched_taxi_data_table()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo de Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# 🌟 Cargar variables de entorno\n",
    "load_dotenv()\n",
    "\n",
    "# 🔐 Configuración de la conexión a la base de datos\n",
    "DB_CONFIG = {\n",
    "    'host': os.getenv(\"DB_HOST\"),\n",
    "    'port': int(os.getenv(\"DB_PORT\", 3306)),\n",
    "    'user': os.getenv(\"DB_USER\"),\n",
    "    'password': os.getenv(\"DB_PASSWORD\"),\n",
    "    'database': os.getenv(\"DB_NAME\")\n",
    "}\n",
    "\n",
    "# Crear el motor de SQLAlchemy\n",
    "engine = create_engine(\n",
    "    f\"mysql+pymysql://{DB_CONFIG['user']}:{DB_CONFIG['password']}@{DB_CONFIG['host']}:{DB_CONFIG['port']}/{DB_CONFIG['database']}\"\n",
    ")\n",
    "\n",
    "# Leer los datos desde la tabla enriched_taxi_data2\n",
    "query = \"SELECT * FROM enriched_taxi_data2\"\n",
    "df = pd.read_sql(query, engine)\n",
    "\n",
    "# Seleccionar las características y la variable objetivo\n",
    "X = df[['PULocationID', 'pickup_weekday', 'pickup_hour']]\n",
    "y = df['trip_count']\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Crear y entrenar el modelo\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Realizar predicciones sobre el conjunto de prueba\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluar el rendimiento del modelo\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"MAE: {mae:.2f}\")\n",
    "print(f\"R²: {r2:.2f}\")\n",
    "\n",
    "# Función para predecir la demanda dada una zona, día y hora\n",
    "def predict_demand(pulocationid, pickup_weekday, pickup_hour):\n",
    "    # Crear un DataFrame con las mismas columnas que el DataFrame original utilizado para entrenar el modelo\n",
    "    input_data = pd.DataFrame({\n",
    "        'PULocationID': [pulocationid],\n",
    "        'pickup_weekday': [pickup_weekday],\n",
    "        'pickup_hour': [pickup_hour]\n",
    "    })\n",
    "    \n",
    "    prediction = model.predict(input_data)\n",
    "    return prediction[0]\n",
    "\n",
    "# Ejemplo de uso de la función predict_demand\n",
    "predicted_demand = predict_demand(84, 3, 12)  # 84 es el PULocationID, 3 es miércoles, 13 es la hora\n",
    "print(f\"Predicted Demand: {predicted_demand:.2f}\")\n",
    "\n",
    "predicted_demand1 = predict_demand(99, 3, 12)  # 99 es el PULocationID, 3 es miércoles, 13 es la hora\n",
    "print(f\"Predicted Demand: {predicted_demand1:.2f}\")\n",
    "\n",
    "predicted_demand2 = predict_demand(204, 3, 12)  # 204 es el PULocationID, 3 es miércoles, 13 es la hora\n",
    "print(f\"Predicted Demand: {predicted_demand2:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Modelo de zonas adjacentes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 3.67\n",
      "R²: 0.03\n",
      "Zone with highest demand: 1, Demand: 5.91\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import ast\n",
    "\n",
    "# 🌟 Cargar variables de entorno\n",
    "load_dotenv()\n",
    "\n",
    "# 🔐 Configuración de la conexión a la base de datos\n",
    "DB_CONFIG = {\n",
    "    'host': os.getenv(\"DB_HOST\"),\n",
    "    'port': int(os.getenv(\"DB_PORT\", 3306)),\n",
    "    'user': os.getenv(\"DB_USER\"),\n",
    "    'password': os.getenv(\"DB_PASSWORD\"),\n",
    "    'database': os.getenv(\"DB_NAME\")\n",
    "}\n",
    "\n",
    "# Crear el motor de SQLAlchemy\n",
    "engine = create_engine(\n",
    "    f\"mysql+pymysql://{DB_CONFIG['user']}:{DB_CONFIG['password']}@{DB_CONFIG['host']}:{DB_CONFIG['port']}/{DB_CONFIG['database']}\"\n",
    ")\n",
    "\n",
    "# Leer los datos desde la tabla enriched_taxi_data2\n",
    "query = \"SELECT * FROM enriched_taxi_data2\"\n",
    "df = pd.read_sql(query, engine)\n",
    "\n",
    "# Leer el archivo de zonas adyacentes\n",
    "adjacent_zones_df = pd.read_csv('adjacent_zones.csv')\n",
    "adjacent_zones_df['adjacent_zones'] = adjacent_zones_df['adjacent_zones'].apply(ast.literal_eval)\n",
    "\n",
    "# Función para obtener las zonas adyacentes\n",
    "def get_adjacent_zones(location_id):\n",
    "    row = adjacent_zones_df.loc[adjacent_zones_df['LocationID'] == location_id]\n",
    "    if not row.empty:\n",
    "        return row.iloc[0]['adjacent_zones']\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "# Seleccionar las características y la variable objetivo\n",
    "X = df[['PULocationID', 'pickup_weekday', 'pickup_hour']]\n",
    "y = df['trip_count']\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Crear y entrenar el modelo\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Realizar predicciones sobre el conjunto de prueba\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluar el rendimiento del modelo\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"MAE: {mae:.2f}\")\n",
    "print(f\"R²: {r2:.2f}\")\n",
    "\n",
    "# Función para predecir la demanda en una zona incluyendo las zonas adyacentes\n",
    "def predict_demand_with_adjacent(pulocationid, pickup_weekday, pickup_hour):\n",
    "    # Obtener las zonas adyacentes\n",
    "    adjacent_zones = get_adjacent_zones(pulocationid)\n",
    "    all_zones = adjacent_zones + [pulocationid]\n",
    "    \n",
    "    # Preparar los datos de entrada para todas las zonas\n",
    "    input_data = pd.DataFrame({\n",
    "        'PULocationID': all_zones,\n",
    "        'pickup_weekday': [pickup_weekday] * len(all_zones),\n",
    "        'pickup_hour': [pickup_hour] * len(all_zones)\n",
    "    })\n",
    "    \n",
    "    # Realizar predicciones para todas las zonas y devolver los resultados individuales\n",
    "    predictions = model.predict(input_data)\n",
    "    result = {zone: prediction for zone, prediction in zip(all_zones, predictions)}\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Ejemplo de uso de la función predict_demand_with_adjacent\n",
    "predicted_demands = predict_demand_with_adjacent(1, 6, 12)  # 260 es el PULocationID, 3 es miércoles, 13 es la hora\n",
    "\n",
    "# Encontrar la zona con mayor demanda\n",
    "max_zone = max(predicted_demands, key=predicted_demands.get)\n",
    "print(f\"Zone with highest demand: {max_zone}, Demand: {predicted_demands[max_zone]:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crear Archivo de Machine Learning PLK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import ast\n",
    "import joblib  # Importar joblib para guardar y cargar el modelo\n",
    "\n",
    "# 🌟 Cargar variables de entorno\n",
    "load_dotenv()\n",
    "\n",
    "# 🔐 Configuración de la conexión a la base de datos\n",
    "DB_CONFIG = {\n",
    "    'host': os.getenv(\"DB_HOST\"),\n",
    "    'port': int(os.getenv(\"DB_PORT\", 3306)),\n",
    "    'user': os.getenv(\"DB_USER\"),\n",
    "    'password': os.getenv(\"DB_PASSWORD\"),\n",
    "    'database': os.getenv(\"DB_NAME\")\n",
    "}\n",
    "\n",
    "# Crear el motor de SQLAlchemy\n",
    "engine = create_engine(\n",
    "    f\"mysql+pymysql://{DB_CONFIG['user']}:{DB_CONFIG['password']}@{DB_CONFIG['host']}:{DB_CONFIG['port']}/{DB_CONFIG['database']}\"\n",
    ")\n",
    "\n",
    "# Leer los datos desde la tabla enriched_taxi_data2\n",
    "query = \"SELECT * FROM enriched_taxi_data2\"\n",
    "df = pd.read_sql(query, engine)\n",
    "\n",
    "# Leer el archivo de zonas adyacentes\n",
    "adjacent_zones_df = pd.read_csv('adjacent_zones.csv')\n",
    "adjacent_zones_df['adjacent_zones'] = adjacent_zones_df['adjacent_zones'].apply(ast.literal_eval)\n",
    "\n",
    "# Función para obtener las zonas adyacentes\n",
    "def get_adjacent_zones(location_id):\n",
    "    row = adjacent_zones_df.loc[adjacent_zones_df['LocationID'] == location_id]\n",
    "    if not row.empty:\n",
    "        return row.iloc[0]['adjacent_zones']\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "# Seleccionar las características y la variable objetivo\n",
    "X = df[['PULocationID', 'pickup_weekday', 'pickup_hour']]\n",
    "y = df['trip_count']\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Crear y entrenar el modelo\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Guardar el modelo entrenado en un archivo\n",
    "joblib.dump(model, 'linear_regression_model.pkl')\n",
    "\n",
    "# Evaluar el rendimiento del modelo\n",
    "y_pred = model.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"MAE: {mae:.2f}\")\n",
    "print(f\"R²: {r2:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba modelo 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚦 Processed batch: 200000/30793760 rows (0.65%) | Last batch time: 414.81s | Est. remaining: 1050.85 mins\n",
      "🚦 Processed batch: 400000/30793760 rows (1.30%) | Last batch time: 373.03s | Est. remaining: 938.80 mins\n",
      "🚦 Processed batch: 600000/30793760 rows (1.95%) | Last batch time: 1637.78s | Est. remaining: 4094.45 mins\n",
      "❌ Error en el procesamiento del lote: (pymysql.err.OperationalError) (2013, 'Lost connection to MySQL server during query ([WinError 10060] Se produjo un error durante el intento de conexión ya que la parte conectada no respondió adecuadamente tras un periodo de tiempo, o bien se produjo un error en la conexión establecida ya que el host conectado no ha podido responder)')\n",
      "[SQL: \n",
      "        SELECT \n",
      "            DATE(t.Pickup_datetime) AS pickup_date,\n",
      "            t.PULocationID,\n",
      "            DAYOFWEEK(t.Pickup_datetime) AS pickup_weekday,\n",
      "            HOUR(t.Pickup_datetime) AS pickup_hour,\n",
      "            COUNT(*) AS trip_count\n",
      "        FROM \n",
      "            taxi_fhv_data1 t\n",
      "        JOIN \n",
      "            taxi_zones z ON t.PULocationID = z.LocationID\n",
      "        WHERE \n",
      "            t.Pickup_datetime >= DATE_SUB(CURDATE(), INTERVAL 44 MONTH)\n",
      "        GROUP BY\n",
      "            pickup_date, t.PULocationID, pickup_weekday, pickup_hour\n",
      "        LIMIT 200000 OFFSET 600000\n",
      "        ]\n",
      "(Background on this error at: https://sqlalche.me/e/20/e3q8)\n",
      "🚦 Processed batch: 600000/30793760 rows (1.95%) | Last batch time: 7212.98s | Est. remaining: 18032.45 mins\n",
      "🚦 Processed batch: 800000/30793760 rows (2.60%) | Last batch time: 344.05s | Est. remaining: 854.40 mins\n",
      "🚦 Processed batch: 922154/30793760 rows (2.99%) | Last batch time: 350.72s | Est. remaining: 870.95 mins\n",
      "🚦 Processed batch: 922154/30793760 rows (2.99%) | Last batch time: 335.28s | Est. remaining: 832.60 mins\n",
      "🚦 Processed batch: 922154/30793760 rows (2.99%) | Last batch time: 324.12s | Est. remaining: 804.91 mins\n",
      "🚦 Processed batch: 922154/30793760 rows (2.99%) | Last batch time: 324.91s | Est. remaining: 806.85 mins\n",
      "🚦 Processed batch: 922154/30793760 rows (2.99%) | Last batch time: 325.11s | Est. remaining: 807.36 mins\n",
      "🚦 Processed batch: 922154/30793760 rows (2.99%) | Last batch time: 328.40s | Est. remaining: 815.52 mins\n",
      "🚦 Processed batch: 922154/30793760 rows (2.99%) | Last batch time: 326.97s | Est. remaining: 811.98 mins\n",
      "🚦 Processed batch: 922154/30793760 rows (2.99%) | Last batch time: 328.40s | Est. remaining: 815.54 mins\n",
      "🚦 Processed batch: 922154/30793760 rows (2.99%) | Last batch time: 324.65s | Est. remaining: 806.21 mins\n",
      "🚦 Processed batch: 922154/30793760 rows (2.99%) | Last batch time: 328.55s | Est. remaining: 815.91 mins\n",
      "🚦 Processed batch: 922154/30793760 rows (2.99%) | Last batch time: 326.56s | Est. remaining: 810.95 mins\n",
      "🚦 Processed batch: 922154/30793760 rows (2.99%) | Last batch time: 326.71s | Est. remaining: 811.34 mins\n",
      "🚦 Processed batch: 922154/30793760 rows (2.99%) | Last batch time: 331.77s | Est. remaining: 823.89 mins\n",
      "🚦 Processed batch: 922154/30793760 rows (2.99%) | Last batch time: 329.43s | Est. remaining: 818.08 mins\n",
      "🚦 Processed batch: 922154/30793760 rows (2.99%) | Last batch time: 328.44s | Est. remaining: 815.64 mins\n",
      "🚦 Processed batch: 922154/30793760 rows (2.99%) | Last batch time: 327.00s | Est. remaining: 812.06 mins\n",
      "🚦 Processed batch: 922154/30793760 rows (2.99%) | Last batch time: 329.82s | Est. remaining: 819.04 mins\n",
      "🚦 Processed batch: 922154/30793760 rows (2.99%) | Last batch time: 332.66s | Est. remaining: 826.10 mins\n",
      "🚦 Processed batch: 922154/30793760 rows (2.99%) | Last batch time: 332.63s | Est. remaining: 826.03 mins\n",
      "🚦 Processed batch: 922154/30793760 rows (2.99%) | Last batch time: 326.65s | Est. remaining: 811.18 mins\n",
      "🚦 Processed batch: 922154/30793760 rows (2.99%) | Last batch time: 325.75s | Est. remaining: 808.95 mins\n",
      "🚦 Processed batch: 922154/30793760 rows (2.99%) | Last batch time: 327.82s | Est. remaining: 814.09 mins\n",
      "🚦 Processed batch: 922154/30793760 rows (2.99%) | Last batch time: 328.14s | Est. remaining: 814.89 mins\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sqlalchemy import create_engine\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "\n",
    "# 🌟 Cargar variables de entorno\n",
    "load_dotenv()\n",
    "\n",
    "# 🔐 Configuración de conexión a la base de datos\n",
    "DB_CONFIG = {\n",
    "    'host': os.getenv(\"DB_HOST\"),\n",
    "    'port': int(os.getenv(\"DB_PORT\", 3306)),\n",
    "    'user': os.getenv(\"DB_USER\"),\n",
    "    'password': os.getenv(\"DB_PASSWORD\"),\n",
    "    'database': os.getenv(\"DB_NAME\")\n",
    "}\n",
    "\n",
    "def fetch_and_process_batch(offset, batch_size):\n",
    "    try:\n",
    "        engine = create_engine(\n",
    "            f\"mysql+pymysql://{DB_CONFIG['user']}:{DB_CONFIG['password']}@{DB_CONFIG['host']}:{DB_CONFIG['port']}/{DB_CONFIG['database']}\"\n",
    "        )\n",
    "        \n",
    "        query = f\"\"\"\n",
    "        SELECT \n",
    "            DATE(t.Pickup_datetime) AS pickup_date,\n",
    "            t.PULocationID,\n",
    "            DAYOFWEEK(t.Pickup_datetime) AS pickup_weekday,\n",
    "            HOUR(t.Pickup_datetime) AS pickup_hour,\n",
    "            COUNT(*) AS trip_count\n",
    "        FROM \n",
    "            taxi_fhv_data1 t\n",
    "        JOIN \n",
    "            taxi_zones z ON t.PULocationID = z.LocationID\n",
    "        WHERE \n",
    "            t.Pickup_datetime >= DATE_SUB(CURDATE(), INTERVAL 44 MONTH)\n",
    "        GROUP BY\n",
    "            pickup_date, t.PULocationID, pickup_weekday, pickup_hour\n",
    "        LIMIT {batch_size} OFFSET {offset}\n",
    "        \"\"\"\n",
    "        \n",
    "        df = pd.read_sql(query, engine)\n",
    "        \n",
    "        df.to_sql('enriched_taxi_data1',\n",
    "                  engine,\n",
    "                  if_exists='append',\n",
    "                  index=False,\n",
    "                  chunksize=30000)\n",
    "        \n",
    "        return len(df)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error en el procesamiento del lote: {e}\")\n",
    "        return 0\n",
    "\n",
    "def populate_enriched_taxi_data_table(batch_size=200000):\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        engine = create_engine(\n",
    "            f\"mysql+pymysql://{DB_CONFIG['user']}:{DB_CONFIG['password']}@{DB_CONFIG['host']}:{DB_CONFIG['port']}/{DB_CONFIG['database']}\"\n",
    "        )\n",
    "        with engine.connect() as connection:\n",
    "            count_query = \"\"\"\n",
    "            SELECT COUNT(*) as total_rows \n",
    "            FROM taxi_fhv_data1 \n",
    "            WHERE Pickup_datetime >= DATE_SUB(CURDATE(), INTERVAL 12 MONTH)\n",
    "            \"\"\"\n",
    "            total_rows = pd.read_sql(count_query, connection)['total_rows'][0]\n",
    "\n",
    "        offsets = range(0, total_rows, batch_size)\n",
    "        processed_rows = 0\n",
    "        \n",
    "        for offset in offsets:\n",
    "            batch_start_time = time.time()\n",
    "            processed = fetch_and_process_batch(offset, batch_size)\n",
    "            processed_rows += processed\n",
    "\n",
    "            # Calcular tiempo de procesamiento del lote y estimar el tiempo restante\n",
    "            batch_time = time.time() - batch_start_time\n",
    "            remaining_batches = (total_rows - processed_rows) // batch_size\n",
    "            estimated_remaining = batch_time * remaining_batches\n",
    "\n",
    "            print(f\"🚦 Processed batch: {processed_rows}/{total_rows} rows \"\n",
    "                  f\"({processed_rows/total_rows*100:.2f}%) \"\n",
    "                  f\"| Last batch time: {batch_time:.2f}s \"\n",
    "                  f\"| Est. remaining: {estimated_remaining/60:.2f} mins\")\n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "        print(f\"✅ Data enrichment completed successfully\")\n",
    "        print(f\"⏱️ Total processing time: {total_time/60:.2f} minutes\")\n",
    "        print(f\"📊 Total rows processed: {processed_rows}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error during data enrichment: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# 🏁 Ejecutar el proceso de enriquecimiento\n",
    "if __name__ == \"__main__\":\n",
    "    populate_enriched_taxi_data_table()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicciones con random forest sobre "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest MAE: 1.74\n",
      "Random Forest R²: 0.74\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# 🌟 Cargar variables de entorno\n",
    "load_dotenv()\n",
    "\n",
    "# 🔐 Configuración de la conexión a la base de datos\n",
    "DB_CONFIG = {\n",
    "    'host': os.getenv(\"DB_HOST\"),\n",
    "    'port': int(os.getenv(\"DB_PORT\", 3306)),\n",
    "    'user': os.getenv(\"DB_USER\"),\n",
    "    'password': os.getenv(\"DB_PASSWORD\"),\n",
    "    'database': os.getenv(\"DB_NAME\")\n",
    "}\n",
    "\n",
    "# Crear el motor de SQLAlchemy para leer datos\n",
    "engine = create_engine(\n",
    "    f\"mysql+pymysql://{DB_CONFIG['user']}:{DB_CONFIG['password']}@{DB_CONFIG['host']}:{DB_CONFIG['port']}/{DB_CONFIG['database']}\"\n",
    ")\n",
    "\n",
    "# Leer los datos desde la tabla enriched_taxi_data2\n",
    "query = \"SELECT * FROM enriched_taxi_data1\"\n",
    "df = pd.read_sql(query, engine)\n",
    "\n",
    "# Seleccionar las características y la variable objetivo\n",
    "X = df[['PULocationID', 'pickup_weekday', 'pickup_hour']]\n",
    "y = df['trip_count']\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Crear y entrenar el modelo de Random Forest\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Realizar predicciones sobre el conjunto de prueba\n",
    "rf_y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluar el rendimiento del modelo\n",
    "rf_mae = mean_absolute_error(y_test, rf_y_pred)\n",
    "rf_r2 = r2_score(y_test, rf_y_pred)\n",
    "\n",
    "print(f\"Random Forest MAE: {rf_mae:.2f}\")\n",
    "print(f\"Random Forest R²: {rf_r2:.2f}\")\n",
    "\n",
    "# Validación cruzada\n",
    "cv_scores = cross_val_score(rf_model, X, y, cv=5, scoring='neg_mean_absolute_error')\n",
    "print(f\"Random Forest CV MAE Scores: {cv_scores}\")\n",
    "print(f\"Random Forest CV MAE Mean: {-np.mean(cv_scores)}\")\n",
    "print(f\"Random Forest CV MAE Std Dev: {np.std(cv_scores)}\")\n",
    "\n",
    "# Función para predecir la demanda dada una zona, día y hora\n",
    "def predict_demand(pulocationid, pickup_weekday, pickup_hour):\n",
    "    # Crear un DataFrame con las mismas columnas que el DataFrame original utilizado para entrenar el modelo\n",
    "    input_data = pd.DataFrame({\n",
    "        'PULocationID': [pulocationid],\n",
    "        'pickup_weekday': [pickup_weekday],\n",
    "        'pickup_hour': [pickup_hour]\n",
    "    })\n",
    "    \n",
    "    prediction = rf_model.predict(input_data)\n",
    "    return prediction[0]\n",
    "\n",
    "# Ejemplo de uso de la función predict_demand\n",
    "predicted_demand = predict_demand(8, 6, 12)  # 84 es el PULocationID, 3 es miércoles, 12 es la hora\n",
    "print(f\"Predicted Demand: {predicted_demand:.2f}\")\n",
    "\n",
    "predicted_demand1 = predict_demand(99, 6, 12)  # 99 es el PULocationID, 3 es miércoles, 12 es la hora\n",
    "print(f\"Predicted Demand: {predicted_demand1:.2f}\")\n",
    "\n",
    "predicted_demand2 = predict_demand(1, 6, 12)  # 204 es el PULocationID, 3 es miércoles, 12 es la hora\n",
    "print(f\"Predicted Demand: {predicted_demand2:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicción: {'max_zone_id': 48, 'max_zone_name': 'Clinton East', 'demand': 5.175719093103936, 'detailed_comparison': [{'zone_id': 48, 'zone_name': 'Clinton East', 'predicted_demand': 5.175719093103936}, {'zone_id': 68, 'zone_name': 'East Chelsea', 'predicted_demand': 4.861944168803092}]}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# URL de la API\n",
    "url = 'http://localhost:9000/predict/'\n",
    "\n",
    "# Datos a enviar en la solicitud\n",
    "data = {\n",
    "    \"PULocationID\": 100,\n",
    "    \"pickup_weekday\": 6,\n",
    "    \"pickup_hour\": 12\n",
    "}\n",
    "\n",
    "# Realizar la solicitud POST\n",
    "response = requests.post(url, json=data)\n",
    "\n",
    "# Verificar la respuesta\n",
    "if response.status_code == 200:\n",
    "    print(\"Predicción:\", response.json())\n",
    "else:\n",
    "    print(\"Error:\", response.status_code, response.text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
