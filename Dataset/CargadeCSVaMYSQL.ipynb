{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script de Importación de Trafico de NYC a MySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import mysql.connector\n",
    "from datetime import datetime\n",
    "\n",
    "# Cargar las variables de entorno desde el archivo .env\n",
    "load_dotenv()\n",
    "\n",
    "# Obtener las variables de entorno\n",
    "db_host = os.getenv('DB_HOST')\n",
    "db_user = os.getenv('DB_USER')\n",
    "db_password = os.getenv('DB_PASSWORD')\n",
    "db_name = os.getenv('DB_NAME')\n",
    "db_port = os.getenv('DB_PORT')\n",
    "csv_file_path = 'Automated_Traffic_Volume_Counts.csv'  # Asegúrate de actualizar esto con tu archivo CSV correcto\n",
    "\n",
    "# Verificar si las variables de entorno se cargaron correctamente\n",
    "if not all([db_host, db_user, db_password, db_name, db_port]):\n",
    "    raise ValueError(\"No se pudieron cargar todas las variables de entorno. Por favor verifica el archivo .env.\")\n",
    "\n",
    "# Cargar el archivo CSV en un DataFrame de pandas\n",
    "try:\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "except FileNotFoundError as e:\n",
    "    raise FileNotFoundError(f\"El archivo {csv_file_path} no se encuentra. Por favor verifica la ruta.\")\n",
    "\n",
    "# Filtrar los datos desde enero 2023 hasta agosto 2024\n",
    "start_date = datetime(2023, 1, 1)\n",
    "end_date = datetime(2024, 8, 31)\n",
    "\n",
    "# Ensure the columns are in the correct format\n",
    "df['Yr'] = df['Yr'].astype(int)\n",
    "df['M'] = df['M'].astype(int)\n",
    "df['D'] = df['D'].astype(int)\n",
    "df['HH'] = df['HH'].astype(int)\n",
    "df['MM'] = df['MM'].astype(int)\n",
    "\n",
    "# Create a datetime column\n",
    "df['datetime'] = pd.to_datetime(df[['Yr', 'M', 'D', 'HH', 'MM']].rename(columns={'Yr': 'year', 'M': 'month', 'D': 'day', 'HH': 'hour', 'MM': 'minute'}))\n",
    "\n",
    "df_filtered = df[(df['datetime'] >= start_date) & (df['datetime'] <= end_date)].copy()\n",
    "\n",
    "# Seleccionar solo las columnas necesarias para el análisis\n",
    "columns_to_keep = ['RequestID', 'Boro', 'Yr', 'M', 'D', 'HH', 'MM', 'Vol', 'SegmentID', 'WktGeom', 'street', 'Direction']\n",
    "df_filtered = df_filtered[columns_to_keep]\n",
    "\n",
    "# Conectar a la base de datos MySQL\n",
    "try:\n",
    "    db_connection = mysql.connector.connect(\n",
    "        host=db_host,\n",
    "        user=db_user,\n",
    "        password=db_password,\n",
    "        database=db_name,\n",
    "        port=int(db_port)\n",
    "    )\n",
    "except mysql.connector.Error as err:\n",
    "    raise mysql.connector.Error(f\"Error al conectar a la base de datos: {err}\")\n",
    "\n",
    "cursor = db_connection.cursor()\n",
    "\n",
    "# Insertar los datos filtrados del DataFrame en bloques de 100,000 filas\n",
    "chunk_size = 100000\n",
    "rows_inserted = 0\n",
    "for start in range(0, len(df_filtered), chunk_size):\n",
    "    end = start + chunk_size\n",
    "    chunk = df_filtered.iloc[start:end]\n",
    "\n",
    "    insert_query = \"\"\"\n",
    "    INSERT INTO trafico (RequestID, Boro, Yr, M, D, HH, MM, Vol, SegmentID, WktGeom, Direction)\n",
    "    VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "    \"\"\"\n",
    "    data = [(\n",
    "        row['RequestID'], row['Boro'], row['Yr'], row['M'], row['D'], row['HH'], row['MM'], row['Vol'],\n",
    "        row['SegmentID'], row['WktGeom'], row['Direction']\n",
    "    ) for _, row in chunk.iterrows()]\n",
    "\n",
    "    cursor.executemany(insert_query, data)\n",
    "    rows_inserted += len(data)\n",
    "    \n",
    "    # Imprimir el progreso solo cada 10 bloques\n",
    "    if start // chunk_size % 10 == 0:\n",
    "        print(f\"Se han insertado {len(data)} filas en este bloque. Total filas insertadas: {rows_inserted}\")\n",
    "\n",
    "# Confirmar los cambios y cerrar la conexión\n",
    "db_connection.commit()\n",
    "cursor.close()\n",
    "db_connection.close()\n",
    "\n",
    "# Mostrar el número total de filas insertadas\n",
    "print(f\"Se han insertado un total de {rows_inserted} filas en la base de datos.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script de Importación de Temperaturas Promedio de NYC a MySQL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import mysql.connector\n",
    "\n",
    "# Cargar las variables de entorno desde el archivo .env\n",
    "load_dotenv()\n",
    "\n",
    "# Obtener las variables de entorno\n",
    "db_host = os.getenv('DB_HOST')\n",
    "db_user = os.getenv('DB_USER')\n",
    "db_password = os.getenv('DB_PASSWORD')\n",
    "db_name = os.getenv('DB_NAME')\n",
    "db_port = os.getenv('DB_PORT')\n",
    "csv_file_path = 'temperaturas_promedio_nyc_mensual.csv'  # Asegúrate de actualizar esto con tu archivo CSV correcto\n",
    "\n",
    "# Verificar si las variables de entorno se cargaron correctamente\n",
    "if not all([db_host, db_user, db_password, db_name, db_port]):\n",
    "    raise ValueError(\"No se pudieron cargar todas las variables de entorno. Por favor verifica el archivo .env.\")\n",
    "\n",
    "# Cargar el archivo CSV en un DataFrame de pandas\n",
    "try:\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "except FileNotFoundError as e:\n",
    "    raise FileNotFoundError(f\"El archivo {csv_file_path} no se encuentra. Por favor verifica la ruta.\")\n",
    "\n",
    "# Conectar a la base de datos MySQL\n",
    "try:\n",
    "    db_connection = mysql.connector.connect(\n",
    "        host=db_host,\n",
    "        user=db_user,\n",
    "        password=db_password,\n",
    "        database=db_name,\n",
    "        port=int(db_port)\n",
    "    )\n",
    "except mysql.connector.Error as err:\n",
    "    raise mysql.connector.Error(f\"Error al conectar a la base de datos: {err}\")\n",
    "\n",
    "cursor = db_connection.cursor()\n",
    "\n",
    "# Insertar los datos del DataFrame en la tabla MySQL\n",
    "insert_query = \"\"\"\n",
    "INSERT INTO temperaturas (Mes, Manhattan, Brooklyn, Queens, The_Bronx, Staten_Island)\n",
    "VALUES (%s, %s, %s, %s, %s, %s)\n",
    "\"\"\"\n",
    "data = [tuple(row) for row in df.values]\n",
    "\n",
    "cursor.executemany(insert_query, data)\n",
    "\n",
    "# Confirmar los cambios y cerrar la conexión\n",
    "db_connection.commit()\n",
    "cursor.close()\n",
    "db_connection.close()\n",
    "\n",
    "# Mostrar el número de filas insertadas\n",
    "print(f\"Se han insertado {len(data)} filas en la base de datos.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script de Importación de Taxi_Zones de NYC a MySQL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sqlalchemy import create_engine, text\n",
    "from dotenv import load_dotenv\n",
    "from shapely import wkt\n",
    "\n",
    "# Cargar las variables de entorno desde el archivo .env\n",
    "load_dotenv()\n",
    "\n",
    "# Configuración de la conexión a la base de datos con credenciales desde variables de entorno\n",
    "DB_CONFIG = {\n",
    "    'host': os.getenv(\"DB_HOST\"),\n",
    "    'port': int(os.getenv(\"DB_PORT\", 3306)),\n",
    "    'user': os.getenv(\"DB_USER\"),\n",
    "    'password': os.getenv(\"DB_PASSWORD\"),\n",
    "    'database': os.getenv(\"DB_NAME\")\n",
    "}\n",
    "\n",
    "def create_and_populate_taxi_zones_table(csv_file_path):\n",
    "    try:\n",
    "        # Leer el archivo CSV en un DataFrame de pandas\n",
    "        df = pd.read_csv(csv_file_path)\n",
    "\n",
    "        # Convertir la columna 'the_geom' a objetos geométricos utilizando shapely\n",
    "        df['geometry'] = df['the_geom'].apply(lambda x: wkt.loads(x) if pd.notnull(x) else None)\n",
    "        \n",
    "        # Eliminar las columnas adicionales que no están en la tabla 'taxi_zones'\n",
    "        df = df[['LocationID', 'borough', 'zone', 'the_geom']]\n",
    "\n",
    "        # Eliminar filas duplicadas basadas en 'LocationID'\n",
    "        df = df.drop_duplicates(subset='LocationID')\n",
    "\n",
    "        # Crear el motor de SQLAlchemy para las operaciones de base de datos\n",
    "        engine = create_engine(\n",
    "            f\"mysql+pymysql://{DB_CONFIG['user']}:{DB_CONFIG['password']}@\"\n",
    "            f\"{DB_CONFIG['host']}:{DB_CONFIG['port']}/{DB_CONFIG['database']}\"\n",
    "        )\n",
    "\n",
    "        # Definir el esquema de la tabla utilizando text\n",
    "        create_table_query = text(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS taxi_zones (\n",
    "            LocationID INT PRIMARY KEY,\n",
    "            Borough VARCHAR(50) NOT NULL,\n",
    "            Zone VARCHAR(100) NOT NULL,\n",
    "            the_geom TEXT NOT NULL\n",
    "        );\n",
    "        \"\"\")\n",
    "\n",
    "        # Ejecutar la creación de la tabla usando el método correcto\n",
    "        with engine.connect() as connection:\n",
    "            connection.execute(create_table_query)\n",
    "\n",
    "        # Guardar los datos transformados en la base de datos\n",
    "        df.to_sql('taxi_zones', engine, if_exists='append', index=False)\n",
    "\n",
    "        print(\"🚀 Tabla taxi_zones creada y datos insertados exitosamente\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error durante la creación o inserción de datos en la tabla taxi_zones: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# Ruta al archivo CSV\n",
    "csv_file_path = 'taxi_zones.csv'\n",
    "\n",
    "# Ejecutar la función para crear la tabla y poblarla con datos\n",
    "if __name__ == \"__main__\":\n",
    "    create_and_populate_taxi_zones_table(csv_file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consultas en MYSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 Total de registros: 1472569\n",
      "✅ Proceso de consulta completado\n",
      "🎉 Conteo exitoso: 1472569 registros\n"
     ]
    }
   ],
   "source": [
    "import mysql.connector\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# 📝 Configuración del logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, \n",
    "    format='%(asctime)s - %(levelname)s: %(message)s',\n",
    "    filename='database_query.log'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class DatabaseManager:\n",
    "    \"\"\"🗄️ Administrador de conexiones y consultas a la base de datos\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # 🔑 Cargar variables de entorno\n",
    "        load_dotenv()\n",
    "        \n",
    "        # 🔒 Obtener credenciales de la base de datos\n",
    "        self.db_host = os.getenv('DB_HOST')\n",
    "        self.db_user = os.getenv('DB_USER')\n",
    "        self.db_password = os.getenv('DB_PASSWORD')\n",
    "        self.db_name = os.getenv('DB_NAME')\n",
    "\n",
    "    def _create_connection(self):\n",
    "        \"\"\"🔌 Crear una conexión a la base de datos\"\"\"\n",
    "        try:\n",
    "            conn = mysql.connector.connect(\n",
    "                host=self.db_host,\n",
    "                user=self.db_user,\n",
    "                password=self.db_password,\n",
    "                database=self.db_name\n",
    "            )\n",
    "            logger.info(\"🟢 Conexión establecida exitosamente\")\n",
    "            return conn\n",
    "        except mysql.connector.Error as err:\n",
    "            logger.error(f\"🔴 Error de conexión: {err}\")\n",
    "            raise\n",
    "\n",
    "    def count_records(self):\n",
    "        \"\"\"📊 Ejecutar un COUNT(1) en la tabla taxi_fhv_data\"\"\"\n",
    "        conn = None\n",
    "        cursor = None\n",
    "        try:\n",
    "            conn = self._create_connection()\n",
    "            cursor = conn.cursor()\n",
    "            \n",
    "            # 🔍 Ejecutar la consulta COUNT(1)\n",
    "            query = \"SELECT COUNT(1) FROM taxi_fhv_data\"\n",
    "            cursor.execute(query)\n",
    "            count = cursor.fetchone()[0]\n",
    "            logger.info(f\"📈 Total de registros: {count}\")\n",
    "            print(f\"📈 Total de registros: {count}\")\n",
    "            return count\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"❌ Error en la consulta: {e}\")\n",
    "            print(f\"❌ Error en la consulta: {e}\")\n",
    "            raise\n",
    "            \n",
    "        finally:\n",
    "            # 🧹 Limpieza de recursos\n",
    "            if cursor:\n",
    "                cursor.close()\n",
    "            if conn:\n",
    "                conn.close()\n",
    "                logger.info(\"🔌 Conexión cerrada\")\n",
    "            \n",
    "            # ✅ Registro de finalización\n",
    "            print(\"✅ Proceso de consulta completado\")\n",
    "            logger.info(\"✅ Proceso de consulta completado\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"🎯 Función principal de ejecución\"\"\"\n",
    "    db_manager = DatabaseManager()\n",
    "    \n",
    "    try:\n",
    "        record_count = db_manager.count_records()\n",
    "        print(f\"🎉 Conteo exitoso: {record_count} registros\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ El proceso falló: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nueva tabla para machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚦 Processed batch: 25000/1472569 rows (1.70%) | Last batch time: 24.54s | Est. remaining: 23.32 mins\n",
      "🚦 Processed batch: 50000/1472569 rows (3.40%) | Last batch time: 26.81s | Est. remaining: 25.02 mins\n",
      "🚦 Processed batch: 75000/1472569 rows (5.09%) | Last batch time: 25.36s | Est. remaining: 23.25 mins\n",
      "🚦 Processed batch: 100000/1472569 rows (6.79%) | Last batch time: 25.96s | Est. remaining: 23.36 mins\n",
      "🚦 Processed batch: 125000/1472569 rows (8.49%) | Last batch time: 28.73s | Est. remaining: 25.37 mins\n",
      "🚦 Processed batch: 150000/1472569 rows (10.19%) | Last batch time: 30.63s | Est. remaining: 26.54 mins\n",
      "🚦 Processed batch: 175000/1472569 rows (11.88%) | Last batch time: 28.11s | Est. remaining: 23.89 mins\n",
      "🚦 Processed batch: 200000/1472569 rows (13.58%) | Last batch time: 26.59s | Est. remaining: 22.16 mins\n",
      "🚦 Processed batch: 225000/1472569 rows (15.28%) | Last batch time: 26.39s | Est. remaining: 21.56 mins\n",
      "🚦 Processed batch: 250000/1472569 rows (16.98%) | Last batch time: 25.51s | Est. remaining: 20.41 mins\n",
      "🚦 Processed batch: 275000/1472569 rows (18.67%) | Last batch time: 25.10s | Est. remaining: 19.66 mins\n",
      "🚦 Processed batch: 300000/1472569 rows (20.37%) | Last batch time: 26.52s | Est. remaining: 20.34 mins\n",
      "🚦 Processed batch: 325000/1472569 rows (22.07%) | Last batch time: 24.84s | Est. remaining: 18.63 mins\n",
      "🚦 Processed batch: 349381/1472569 rows (23.73%) | Last batch time: 28.38s | Est. remaining: 20.81 mins\n",
      "🚦 Processed batch: 349381/1472569 rows (23.73%) | Last batch time: 23.08s | Est. remaining: 16.92 mins\n",
      "🚦 Processed batch: 349381/1472569 rows (23.73%) | Last batch time: 20.69s | Est. remaining: 15.17 mins\n",
      "🚦 Processed batch: 349381/1472569 rows (23.73%) | Last batch time: 19.20s | Est. remaining: 14.08 mins\n",
      "🚦 Processed batch: 349381/1472569 rows (23.73%) | Last batch time: 21.45s | Est. remaining: 15.73 mins\n",
      "🚦 Processed batch: 349381/1472569 rows (23.73%) | Last batch time: 18.97s | Est. remaining: 13.91 mins\n",
      "🚦 Processed batch: 349381/1472569 rows (23.73%) | Last batch time: 20.01s | Est. remaining: 14.68 mins\n",
      "🚦 Processed batch: 349381/1472569 rows (23.73%) | Last batch time: 19.10s | Est. remaining: 14.01 mins\n",
      "🚦 Processed batch: 349381/1472569 rows (23.73%) | Last batch time: 19.97s | Est. remaining: 14.64 mins\n",
      "🚦 Processed batch: 349381/1472569 rows (23.73%) | Last batch time: 19.47s | Est. remaining: 14.28 mins\n",
      "🚦 Processed batch: 349381/1472569 rows (23.73%) | Last batch time: 19.07s | Est. remaining: 13.99 mins\n",
      "🚦 Processed batch: 349381/1472569 rows (23.73%) | Last batch time: 20.28s | Est. remaining: 14.87 mins\n",
      "🚦 Processed batch: 349381/1472569 rows (23.73%) | Last batch time: 20.79s | Est. remaining: 15.25 mins\n",
      "🚦 Processed batch: 349381/1472569 rows (23.73%) | Last batch time: 19.66s | Est. remaining: 14.41 mins\n",
      "🚦 Processed batch: 349381/1472569 rows (23.73%) | Last batch time: 20.83s | Est. remaining: 15.28 mins\n",
      "🚦 Processed batch: 349381/1472569 rows (23.73%) | Last batch time: 22.37s | Est. remaining: 16.40 mins\n",
      "🚦 Processed batch: 349381/1472569 rows (23.73%) | Last batch time: 20.68s | Est. remaining: 15.17 mins\n",
      "🚦 Processed batch: 349381/1472569 rows (23.73%) | Last batch time: 19.18s | Est. remaining: 14.07 mins\n",
      "🚦 Processed batch: 349381/1472569 rows (23.73%) | Last batch time: 20.85s | Est. remaining: 15.29 mins\n",
      "🚦 Processed batch: 349381/1472569 rows (23.73%) | Last batch time: 20.39s | Est. remaining: 14.95 mins\n",
      "🚦 Processed batch: 349381/1472569 rows (23.73%) | Last batch time: 20.50s | Est. remaining: 15.04 mins\n",
      "🚦 Processed batch: 349381/1472569 rows (23.73%) | Last batch time: 21.30s | Est. remaining: 15.62 mins\n",
      "🚦 Processed batch: 349381/1472569 rows (23.73%) | Last batch time: 19.81s | Est. remaining: 14.53 mins\n",
      "🚦 Processed batch: 349381/1472569 rows (23.73%) | Last batch time: 19.37s | Est. remaining: 14.21 mins\n",
      "🚦 Processed batch: 349381/1472569 rows (23.73%) | Last batch time: 19.72s | Est. remaining: 14.46 mins\n",
      "🚦 Processed batch: 349381/1472569 rows (23.73%) | Last batch time: 19.18s | Est. remaining: 14.07 mins\n",
      "🚦 Processed batch: 349381/1472569 rows (23.73%) | Last batch time: 20.65s | Est. remaining: 15.14 mins\n",
      "🚦 Processed batch: 349381/1472569 rows (23.73%) | Last batch time: 20.62s | Est. remaining: 15.12 mins\n",
      "🚦 Processed batch: 349381/1472569 rows (23.73%) | Last batch time: 20.33s | Est. remaining: 14.91 mins\n",
      "🚦 Processed batch: 349381/1472569 rows (23.73%) | Last batch time: 19.60s | Est. remaining: 14.37 mins\n",
      "🚦 Processed batch: 349381/1472569 rows (23.73%) | Last batch time: 22.78s | Est. remaining: 16.71 mins\n",
      "🚦 Processed batch: 349381/1472569 rows (23.73%) | Last batch time: 19.21s | Est. remaining: 14.09 mins\n",
      "🚦 Processed batch: 349381/1472569 rows (23.73%) | Last batch time: 20.55s | Est. remaining: 15.07 mins\n",
      "🚦 Processed batch: 349381/1472569 rows (23.73%) | Last batch time: 21.44s | Est. remaining: 15.72 mins\n",
      "🚦 Processed batch: 349381/1472569 rows (23.73%) | Last batch time: 21.00s | Est. remaining: 15.40 mins\n",
      "🚦 Processed batch: 349381/1472569 rows (23.73%) | Last batch time: 19.44s | Est. remaining: 14.26 mins\n",
      "🚦 Processed batch: 349381/1472569 rows (23.73%) | Last batch time: 20.61s | Est. remaining: 15.11 mins\n",
      "🚦 Processed batch: 349381/1472569 rows (23.73%) | Last batch time: 21.07s | Est. remaining: 15.45 mins\n",
      "🚦 Processed batch: 349381/1472569 rows (23.73%) | Last batch time: 20.73s | Est. remaining: 15.20 mins\n",
      "🚦 Processed batch: 349381/1472569 rows (23.73%) | Last batch time: 19.53s | Est. remaining: 14.32 mins\n",
      "🚦 Processed batch: 349381/1472569 rows (23.73%) | Last batch time: 19.95s | Est. remaining: 14.63 mins\n",
      "🚦 Processed batch: 349381/1472569 rows (23.73%) | Last batch time: 20.79s | Est. remaining: 15.25 mins\n",
      "🚦 Processed batch: 349381/1472569 rows (23.73%) | Last batch time: 19.76s | Est. remaining: 14.49 mins\n",
      "🚦 Processed batch: 349381/1472569 rows (23.73%) | Last batch time: 20.11s | Est. remaining: 14.75 mins\n",
      "🚦 Processed batch: 349381/1472569 rows (23.73%) | Last batch time: 22.12s | Est. remaining: 16.22 mins\n",
      "🚦 Processed batch: 349381/1472569 rows (23.73%) | Last batch time: 20.87s | Est. remaining: 15.31 mins\n",
      "✅ Data enrichment completed successfully\n",
      "⏱️ Total processing time: 21.57 minutes\n",
      "📊 Total rows processed: 349381\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sqlalchemy import create_engine\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "\n",
    "# 🌟 Cargar variables de entorno\n",
    "load_dotenv()\n",
    "\n",
    "# 🔐 Configuración de conexión a la base de datos\n",
    "DB_CONFIG = {\n",
    "    'host': os.getenv(\"DB_HOST\"),\n",
    "    'port': int(os.getenv(\"DB_PORT\", 3306)),\n",
    "    'user': os.getenv(\"DB_USER\"),\n",
    "    'password': os.getenv(\"DB_PASSWORD\"),\n",
    "    'database': os.getenv(\"DB_NAME\")\n",
    "}\n",
    "\n",
    "def fetch_and_process_batch(offset, batch_size):\n",
    "    try:\n",
    "        engine = create_engine(\n",
    "            f\"mysql+pymysql://{DB_CONFIG['user']}:{DB_CONFIG['password']}@{DB_CONFIG['host']}:{DB_CONFIG['port']}/{DB_CONFIG['database']}\"\n",
    "        )\n",
    "        \n",
    "        query = f\"\"\"\n",
    "        SELECT \n",
    "            DATE(t.Pickup_datetime) AS pickup_date,\n",
    "            t.PULocationID,\n",
    "            z.Borough AS pickup_borough,\n",
    "            DAY(t.Pickup_datetime) AS pickup_day,\n",
    "            HOUR(t.Pickup_datetime) AS pickup_hour,\n",
    "            COUNT(*) AS trip_count\n",
    "        FROM \n",
    "            taxi_fhv_data t\n",
    "        JOIN \n",
    "            taxi_zones z ON t.PULocationID = z.LocationID\n",
    "        WHERE \n",
    "            t.Pickup_datetime >= DATE_SUB(CURDATE(), INTERVAL 44 MONTH)\n",
    "        GROUP BY\n",
    "            pickup_date, t.PULocationID, pickup_borough, pickup_day, pickup_hour\n",
    "        LIMIT {batch_size} OFFSET {offset}\n",
    "        \"\"\"\n",
    "        \n",
    "        df = pd.read_sql(query, engine)\n",
    "        \n",
    "        df.to_sql('enriched_taxi_data',\n",
    "                  engine,\n",
    "                  if_exists='append',\n",
    "                  index=False,\n",
    "                  chunksize=25000)\n",
    "        \n",
    "        return len(df)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error en el procesamiento del lote: {e}\")\n",
    "        return 0\n",
    "\n",
    "def populate_enriched_taxi_data_table(batch_size=25000):\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        engine = create_engine(\n",
    "            f\"mysql+pymysql://{DB_CONFIG['user']}:{DB_CONFIG['password']}@{DB_CONFIG['host']}:{DB_CONFIG['port']}/{DB_CONFIG['database']}\"\n",
    "        )\n",
    "        with engine.connect() as connection:\n",
    "            count_query = \"\"\"\n",
    "            SELECT COUNT(*) as total_rows \n",
    "            FROM taxi_fhv_data \n",
    "            WHERE Pickup_datetime >= DATE_SUB(CURDATE(), INTERVAL 44 MONTH)\n",
    "            \"\"\"\n",
    "            total_rows = pd.read_sql(count_query, connection)['total_rows'][0]\n",
    "\n",
    "        offsets = range(0, total_rows, batch_size)\n",
    "        processed_rows = 0\n",
    "        \n",
    "        for offset in offsets:\n",
    "            batch_start_time = time.time()\n",
    "            processed = fetch_and_process_batch(offset, batch_size)\n",
    "            processed_rows += processed\n",
    "\n",
    "            # Calcular tiempo de procesamiento del lote y estimar el tiempo restante\n",
    "            batch_time = time.time() - batch_start_time\n",
    "            remaining_batches = (total_rows - processed_rows) // batch_size\n",
    "            estimated_remaining = batch_time * remaining_batches\n",
    "\n",
    "            print(f\"🚦 Processed batch: {processed_rows}/{total_rows} rows \"\n",
    "                  f\"({processed_rows/total_rows*100:.2f}%) \"\n",
    "                  f\"| Last batch time: {batch_time:.2f}s \"\n",
    "                  f\"| Est. remaining: {estimated_remaining/60:.2f} mins\")\n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "        print(f\"✅ Data enrichment completed successfully\")\n",
    "        print(f\"⏱️ Total processing time: {total_time/60:.2f} minutes\")\n",
    "        print(f\"📊 Total rows processed: {processed_rows}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error during data enrichment: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# 🏁 Ejecutar el proceso de enriquecimiento\n",
    "if __name__ == \"__main__\":\n",
    "    populate_enriched_taxi_data_table()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 3.46\n",
      "R²: 0.14\n",
      "Predicted Demand: 9.20\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# 🌟 Cargar variables de entorno\n",
    "load_dotenv()\n",
    "\n",
    "# 🔐 Configuración de la conexión a la base de datos\n",
    "DB_CONFIG = {\n",
    "    'host': os.getenv(\"DB_HOST\"),\n",
    "    'port': int(os.getenv(\"DB_PORT\", 3306)),\n",
    "    'user': os.getenv(\"DB_USER\"),\n",
    "    'password': os.getenv(\"DB_PASSWORD\"),\n",
    "    'database': os.getenv(\"DB_NAME\")\n",
    "}\n",
    "\n",
    "# Crear el motor de SQLAlchemy\n",
    "engine = create_engine(\n",
    "    f\"mysql+pymysql://{DB_CONFIG['user']}:{DB_CONFIG['password']}@{DB_CONFIG['host']}:{DB_CONFIG['port']}/{DB_CONFIG['database']}\"\n",
    ")\n",
    "\n",
    "# Leer los datos desde la tabla enriched_taxi_data\n",
    "query = \"SELECT * FROM enriched_taxi_data\"\n",
    "df = pd.read_sql(query, engine)\n",
    "\n",
    "# Seleccionar las características y la variable objetivo\n",
    "X = df[['PULocationID', 'pickup_borough', 'pickup_day', 'pickup_hour']]\n",
    "y = df['trip_count']\n",
    "\n",
    "# Convertir variable categórica 'pickup_borough' en variables dummy\n",
    "X = pd.get_dummies(X, columns=['pickup_borough'], drop_first=True)\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Crear y entrenar el modelo\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Realizar predicciones sobre el conjunto de prueba\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluar el rendimiento del modelo\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"MAE: {mae:.2f}\")\n",
    "print(f\"R²: {r2:.2f}\")\n",
    "\n",
    "# Función para predecir la demanda dada una zona, día y hora\n",
    "def predict_demand(pulocationid, pickup_day, pickup_hour, pickup_borough):\n",
    "    # Crear un DataFrame con las mismas columnas que el DataFrame original utilizado para entrenar el modelo\n",
    "    input_data = pd.DataFrame({\n",
    "        'PULocationID': [pulocationid],\n",
    "        'pickup_day': [pickup_day],\n",
    "        'pickup_hour': [pickup_hour]\n",
    "    })\n",
    "\n",
    "    # Añadir columnas dummy para 'pickup_borough'\n",
    "    borough_columns = [col for col in X.columns if col.startswith('pickup_borough_')]\n",
    "    for col in borough_columns:\n",
    "        input_data[col] = 0\n",
    "    \n",
    "    # Establecer el valor adecuado de la columna dummy correspondiente al 'pickup_borough'\n",
    "    if f'pickup_borough_{pickup_borough}' in input_data.columns:\n",
    "        input_data[f'pickup_borough_{pickup_borough}'] = 1\n",
    "    \n",
    "    prediction = model.predict(input_data)\n",
    "    return prediction[0]\n",
    "\n",
    "# Ejemplo de uso de la función predict_demand\n",
    "predicted_demand = predict_demand(11, 11, 13, 'Manhattan')\n",
    "print(f\"Predicted Demand: {predicted_demand:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚦 Processed batch: 25000/1472569 rows (1.70%) | Last batch time: 24.33s | Est. remaining: 23.11 mins\n",
      "🚦 Processed batch: 50000/1472569 rows (3.40%) | Last batch time: 20.45s | Est. remaining: 19.09 mins\n",
      "🚦 Processed batch: 75000/1472569 rows (5.09%) | Last batch time: 14.40s | Est. remaining: 13.20 mins\n",
      "🚦 Processed batch: 100000/1472569 rows (6.79%) | Last batch time: 16.22s | Est. remaining: 14.59 mins\n",
      "🚦 Processed batch: 125000/1472569 rows (8.49%) | Last batch time: 19.26s | Est. remaining: 17.01 mins\n",
      "🚦 Processed batch: 150000/1472569 rows (10.19%) | Last batch time: 16.21s | Est. remaining: 14.05 mins\n",
      "🚦 Processed batch: 175000/1472569 rows (11.88%) | Last batch time: 18.15s | Est. remaining: 15.43 mins\n",
      "🚦 Processed batch: 200000/1472569 rows (13.58%) | Last batch time: 18.38s | Est. remaining: 15.32 mins\n",
      "🚦 Processed batch: 225000/1472569 rows (15.28%) | Last batch time: 18.26s | Est. remaining: 14.91 mins\n",
      "🚦 Processed batch: 250000/1472569 rows (16.98%) | Last batch time: 16.89s | Est. remaining: 13.51 mins\n",
      "🚦 Processed batch: 275000/1472569 rows (18.67%) | Last batch time: 18.95s | Est. remaining: 14.84 mins\n",
      "🚦 Processed batch: 300000/1472569 rows (20.37%) | Last batch time: 18.09s | Est. remaining: 13.87 mins\n",
      "🚦 Processed batch: 325000/1472569 rows (22.07%) | Last batch time: 18.02s | Est. remaining: 13.52 mins\n",
      "🚦 Processed batch: 349381/1472569 rows (23.73%) | Last batch time: 17.91s | Est. remaining: 13.13 mins\n",
      "🚦 Processed batch: 349381/1472569 rows (23.73%) | Last batch time: 11.93s | Est. remaining: 8.75 mins\n",
      "🚦 Processed batch: 349381/1472569 rows (23.73%) | Last batch time: 11.87s | Est. remaining: 8.71 mins\n",
      "🚦 Processed batch: 349381/1472569 rows (23.73%) | Last batch time: 10.93s | Est. remaining: 8.01 mins\n",
      "🚦 Processed batch: 349381/1472569 rows (23.73%) | Last batch time: 12.30s | Est. remaining: 9.02 mins\n",
      "🚦 Processed batch: 349381/1472569 rows (23.73%) | Last batch time: 11.61s | Est. remaining: 8.52 mins\n",
      "🚦 Processed batch: 349381/1472569 rows (23.73%) | Last batch time: 12.12s | Est. remaining: 8.89 mins\n",
      "🚦 Processed batch: 349381/1472569 rows (23.73%) | Last batch time: 11.40s | Est. remaining: 8.36 mins\n",
      "🚦 Processed batch: 349381/1472569 rows (23.73%) | Last batch time: 10.87s | Est. remaining: 7.97 mins\n",
      "🚦 Processed batch: 349381/1472569 rows (23.73%) | Last batch time: 12.85s | Est. remaining: 9.42 mins\n",
      "🚦 Processed batch: 349381/1472569 rows (23.73%) | Last batch time: 11.39s | Est. remaining: 8.35 mins\n",
      "🚦 Processed batch: 349381/1472569 rows (23.73%) | Last batch time: 15.27s | Est. remaining: 11.20 mins\n",
      "🚦 Processed batch: 349381/1472569 rows (23.73%) | Last batch time: 12.33s | Est. remaining: 9.04 mins\n",
      "🚦 Processed batch: 349381/1472569 rows (23.73%) | Last batch time: 12.56s | Est. remaining: 9.21 mins\n",
      "🚦 Processed batch: 349381/1472569 rows (23.73%) | Last batch time: 12.87s | Est. remaining: 9.44 mins\n",
      "🚦 Processed batch: 349381/1472569 rows (23.73%) | Last batch time: 11.04s | Est. remaining: 8.09 mins\n",
      "🚦 Processed batch: 349381/1472569 rows (23.73%) | Last batch time: 11.91s | Est. remaining: 8.74 mins\n",
      "🚦 Processed batch: 349381/1472569 rows (23.73%) | Last batch time: 10.58s | Est. remaining: 7.76 mins\n",
      "🚦 Processed batch: 349381/1472569 rows (23.73%) | Last batch time: 12.65s | Est. remaining: 9.28 mins\n",
      "🚦 Processed batch: 349381/1472569 rows (23.73%) | Last batch time: 11.41s | Est. remaining: 8.37 mins\n",
      "🚦 Processed batch: 349381/1472569 rows (23.73%) | Last batch time: 11.33s | Est. remaining: 8.31 mins\n",
      "🚦 Processed batch: 349381/1472569 rows (23.73%) | Last batch time: 13.88s | Est. remaining: 10.18 mins\n",
      "🚦 Processed batch: 349381/1472569 rows (23.73%) | Last batch time: 10.96s | Est. remaining: 8.04 mins\n",
      "🚦 Processed batch: 349381/1472569 rows (23.73%) | Last batch time: 12.26s | Est. remaining: 8.99 mins\n",
      "🚦 Processed batch: 349381/1472569 rows (23.73%) | Last batch time: 11.97s | Est. remaining: 8.78 mins\n",
      "🚦 Processed batch: 349381/1472569 rows (23.73%) | Last batch time: 12.08s | Est. remaining: 8.86 mins\n",
      "🚦 Processed batch: 349381/1472569 rows (23.73%) | Last batch time: 12.08s | Est. remaining: 8.86 mins\n",
      "🚦 Processed batch: 349381/1472569 rows (23.73%) | Last batch time: 10.65s | Est. remaining: 7.81 mins\n",
      "🚦 Processed batch: 349381/1472569 rows (23.73%) | Last batch time: 11.94s | Est. remaining: 8.75 mins\n",
      "🚦 Processed batch: 349381/1472569 rows (23.73%) | Last batch time: 11.46s | Est. remaining: 8.41 mins\n",
      "🚦 Processed batch: 349381/1472569 rows (23.73%) | Last batch time: 11.80s | Est. remaining: 8.65 mins\n",
      "🚦 Processed batch: 349381/1472569 rows (23.73%) | Last batch time: 12.40s | Est. remaining: 9.09 mins\n",
      "🚦 Processed batch: 349381/1472569 rows (23.73%) | Last batch time: 11.97s | Est. remaining: 8.77 mins\n",
      "🚦 Processed batch: 349381/1472569 rows (23.73%) | Last batch time: 11.12s | Est. remaining: 8.15 mins\n",
      "🚦 Processed batch: 349381/1472569 rows (23.73%) | Last batch time: 11.97s | Est. remaining: 8.78 mins\n",
      "🚦 Processed batch: 349381/1472569 rows (23.73%) | Last batch time: 12.64s | Est. remaining: 9.27 mins\n",
      "🚦 Processed batch: 349381/1472569 rows (23.73%) | Last batch time: 16.86s | Est. remaining: 12.36 mins\n",
      "🚦 Processed batch: 349381/1472569 rows (23.73%) | Last batch time: 11.94s | Est. remaining: 8.76 mins\n",
      "🚦 Processed batch: 349381/1472569 rows (23.73%) | Last batch time: 11.24s | Est. remaining: 8.24 mins\n",
      "🚦 Processed batch: 349381/1472569 rows (23.73%) | Last batch time: 12.66s | Est. remaining: 9.28 mins\n",
      "🚦 Processed batch: 349381/1472569 rows (23.73%) | Last batch time: 11.29s | Est. remaining: 8.28 mins\n",
      "🚦 Processed batch: 349381/1472569 rows (23.73%) | Last batch time: 11.40s | Est. remaining: 8.36 mins\n",
      "🚦 Processed batch: 349381/1472569 rows (23.73%) | Last batch time: 11.75s | Est. remaining: 8.62 mins\n",
      "🚦 Processed batch: 349381/1472569 rows (23.73%) | Last batch time: 11.65s | Est. remaining: 8.54 mins\n",
      "🚦 Processed batch: 349381/1472569 rows (23.73%) | Last batch time: 12.13s | Est. remaining: 8.89 mins\n",
      "🚦 Processed batch: 349381/1472569 rows (23.73%) | Last batch time: 10.97s | Est. remaining: 8.05 mins\n",
      "✅ Data enrichment completed successfully\n",
      "⏱️ Total processing time: 13.36 minutes\n",
      "📊 Total rows processed: 349381\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sqlalchemy import create_engine\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "\n",
    "# 🌟 Cargar variables de entorno\n",
    "load_dotenv()\n",
    "\n",
    "# 🔐 Configuración de conexión a la base de datos\n",
    "DB_CONFIG = {\n",
    "    'host': os.getenv(\"DB_HOST\"),\n",
    "    'port': int(os.getenv(\"DB_PORT\", 3306)),\n",
    "    'user': os.getenv(\"DB_USER\"),\n",
    "    'password': os.getenv(\"DB_PASSWORD\"),\n",
    "    'database': os.getenv(\"DB_NAME\")\n",
    "}\n",
    "\n",
    "def fetch_and_process_batch(offset, batch_size):\n",
    "    try:\n",
    "        engine = create_engine(\n",
    "            f\"mysql+pymysql://{DB_CONFIG['user']}:{DB_CONFIG['password']}@{DB_CONFIG['host']}:{DB_CONFIG['port']}/{DB_CONFIG['database']}\"\n",
    "        )\n",
    "        \n",
    "        query = f\"\"\"\n",
    "        SELECT \n",
    "            DATE(t.Pickup_datetime) AS pickup_date,\n",
    "            t.PULocationID,\n",
    "            DAYOFWEEK(t.Pickup_datetime) AS pickup_weekday,\n",
    "            HOUR(t.Pickup_datetime) AS pickup_hour,\n",
    "            COUNT(*) AS trip_count\n",
    "        FROM \n",
    "            taxi_fhv_data t\n",
    "        JOIN \n",
    "            taxi_zones z ON t.PULocationID = z.LocationID\n",
    "        WHERE \n",
    "            t.Pickup_datetime >= DATE_SUB(CURDATE(), INTERVAL 44 MONTH)\n",
    "        GROUP BY\n",
    "            pickup_date, t.PULocationID, pickup_weekday, pickup_hour\n",
    "        LIMIT {batch_size} OFFSET {offset}\n",
    "        \"\"\"\n",
    "        \n",
    "        df = pd.read_sql(query, engine)\n",
    "        \n",
    "        df.to_sql('enriched_taxi_data2',\n",
    "                  engine,\n",
    "                  if_exists='append',\n",
    "                  index=False,\n",
    "                  chunksize=25000)\n",
    "        \n",
    "        return len(df)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error en el procesamiento del lote: {e}\")\n",
    "        return 0\n",
    "\n",
    "def populate_enriched_taxi_data_table(batch_size=25000):\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        engine = create_engine(\n",
    "            f\"mysql+pymysql://{DB_CONFIG['user']}:{DB_CONFIG['password']}@{DB_CONFIG['host']}:{DB_CONFIG['port']}/{DB_CONFIG['database']}\"\n",
    "        )\n",
    "        with engine.connect() as connection:\n",
    "            count_query = \"\"\"\n",
    "            SELECT COUNT(*) as total_rows \n",
    "            FROM taxi_fhv_data \n",
    "            WHERE Pickup_datetime >= DATE_SUB(CURDATE(), INTERVAL 44 MONTH)\n",
    "            \"\"\"\n",
    "            total_rows = pd.read_sql(count_query, connection)['total_rows'][0]\n",
    "\n",
    "        offsets = range(0, total_rows, batch_size)\n",
    "        processed_rows = 0\n",
    "        \n",
    "        for offset in offsets:\n",
    "            batch_start_time = time.time()\n",
    "            processed = fetch_and_process_batch(offset, batch_size)\n",
    "            processed_rows += processed\n",
    "\n",
    "            # Calcular tiempo de procesamiento del lote y estimar el tiempo restante\n",
    "            batch_time = time.time() - batch_start_time\n",
    "            remaining_batches = (total_rows - processed_rows) // batch_size\n",
    "            estimated_remaining = batch_time * remaining_batches\n",
    "\n",
    "            print(f\"🚦 Processed batch: {processed_rows}/{total_rows} rows \"\n",
    "                  f\"({processed_rows/total_rows*100:.2f}%) \"\n",
    "                  f\"| Last batch time: {batch_time:.2f}s \"\n",
    "                  f\"| Est. remaining: {estimated_remaining/60:.2f} mins\")\n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "        print(f\"✅ Data enrichment completed successfully\")\n",
    "        print(f\"⏱️ Total processing time: {total_time/60:.2f} minutes\")\n",
    "        print(f\"📊 Total rows processed: {processed_rows}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error during data enrichment: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# 🏁 Ejecutar el proceso de enriquecimiento\n",
    "if __name__ == \"__main__\":\n",
    "    populate_enriched_taxi_data_table()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 3.67\n",
      "R²: 0.03\n",
      "Predicted Demand: 4.63\n",
      "Predicted Demand: 4.40\n",
      "Predicted Demand: 2.75\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# 🌟 Cargar variables de entorno\n",
    "load_dotenv()\n",
    "\n",
    "# 🔐 Configuración de la conexión a la base de datos\n",
    "DB_CONFIG = {\n",
    "    'host': os.getenv(\"DB_HOST\"),\n",
    "    'port': int(os.getenv(\"DB_PORT\", 3306)),\n",
    "    'user': os.getenv(\"DB_USER\"),\n",
    "    'password': os.getenv(\"DB_PASSWORD\"),\n",
    "    'database': os.getenv(\"DB_NAME\")\n",
    "}\n",
    "\n",
    "# Crear el motor de SQLAlchemy\n",
    "engine = create_engine(\n",
    "    f\"mysql+pymysql://{DB_CONFIG['user']}:{DB_CONFIG['password']}@{DB_CONFIG['host']}:{DB_CONFIG['port']}/{DB_CONFIG['database']}\"\n",
    ")\n",
    "\n",
    "# Leer los datos desde la tabla enriched_taxi_data2\n",
    "query = \"SELECT * FROM enriched_taxi_data2\"\n",
    "df = pd.read_sql(query, engine)\n",
    "\n",
    "# Seleccionar las características y la variable objetivo\n",
    "X = df[['PULocationID', 'pickup_weekday', 'pickup_hour']]\n",
    "y = df['trip_count']\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Crear y entrenar el modelo\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Realizar predicciones sobre el conjunto de prueba\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluar el rendimiento del modelo\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"MAE: {mae:.2f}\")\n",
    "print(f\"R²: {r2:.2f}\")\n",
    "\n",
    "# Función para predecir la demanda dada una zona, día y hora\n",
    "def predict_demand(pulocationid, pickup_weekday, pickup_hour):\n",
    "    # Crear un DataFrame con las mismas columnas que el DataFrame original utilizado para entrenar el modelo\n",
    "    input_data = pd.DataFrame({\n",
    "        'PULocationID': [pulocationid],\n",
    "        'pickup_weekday': [pickup_weekday],\n",
    "        'pickup_hour': [pickup_hour]\n",
    "    })\n",
    "    \n",
    "    prediction = model.predict(input_data)\n",
    "    return prediction[0]\n",
    "\n",
    "# Ejemplo de uso de la función predict_demand\n",
    "predicted_demand = predict_demand(84, 3, 12)  # 84 es el PULocationID, 3 es miércoles, 13 es la hora\n",
    "print(f\"Predicted Demand: {predicted_demand:.2f}\")\n",
    "\n",
    "predicted_demand1 = predict_demand(99, 3, 12)  # 99 es el PULocationID, 3 es miércoles, 13 es la hora\n",
    "print(f\"Predicted Demand: {predicted_demand1:.2f}\")\n",
    "\n",
    "predicted_demand2 = predict_demand(204, 3, 12)  # 204 es el PULocationID, 3 es miércoles, 13 es la hora\n",
    "print(f\"Predicted Demand: {predicted_demand2:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 3.67\n",
      "R²: 0.03\n",
      "Zone with highest demand: 21, Demand: 6.25\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import ast\n",
    "\n",
    "# 🌟 Cargar variables de entorno\n",
    "load_dotenv()\n",
    "\n",
    "# 🔐 Configuración de la conexión a la base de datos\n",
    "DB_CONFIG = {\n",
    "    'host': os.getenv(\"DB_HOST\"),\n",
    "    'port': int(os.getenv(\"DB_PORT\", 3306)),\n",
    "    'user': os.getenv(\"DB_USER\"),\n",
    "    'password': os.getenv(\"DB_PASSWORD\"),\n",
    "    'database': os.getenv(\"DB_NAME\")\n",
    "}\n",
    "\n",
    "# Crear el motor de SQLAlchemy\n",
    "engine = create_engine(\n",
    "    f\"mysql+pymysql://{DB_CONFIG['user']}:{DB_CONFIG['password']}@{DB_CONFIG['host']}:{DB_CONFIG['port']}/{DB_CONFIG['database']}\"\n",
    ")\n",
    "\n",
    "# Leer los datos desde la tabla enriched_taxi_data2\n",
    "query = \"SELECT * FROM enriched_taxi_data2\"\n",
    "df = pd.read_sql(query, engine)\n",
    "\n",
    "# Leer el archivo de zonas adyacentes\n",
    "adjacent_zones_df = pd.read_csv('adjacent_zones.csv')\n",
    "adjacent_zones_df['adjacent_zones'] = adjacent_zones_df['adjacent_zones'].apply(ast.literal_eval)\n",
    "\n",
    "# Función para obtener las zonas adyacentes\n",
    "def get_adjacent_zones(location_id):\n",
    "    row = adjacent_zones_df.loc[adjacent_zones_df['LocationID'] == location_id]\n",
    "    if not row.empty:\n",
    "        return row.iloc[0]['adjacent_zones']\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "# Seleccionar las características y la variable objetivo\n",
    "X = df[['PULocationID', 'pickup_weekday', 'pickup_hour']]\n",
    "y = df['trip_count']\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Crear y entrenar el modelo\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Realizar predicciones sobre el conjunto de prueba\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluar el rendimiento del modelo\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"MAE: {mae:.2f}\")\n",
    "print(f\"R²: {r2:.2f}\")\n",
    "\n",
    "# Función para predecir la demanda en una zona incluyendo las zonas adyacentes\n",
    "def predict_demand_with_adjacent(pulocationid, pickup_weekday, pickup_hour):\n",
    "    # Obtener las zonas adyacentes\n",
    "    adjacent_zones = get_adjacent_zones(pulocationid)\n",
    "    all_zones = adjacent_zones + [pulocationid]\n",
    "    \n",
    "    # Preparar los datos de entrada para todas las zonas\n",
    "    input_data = pd.DataFrame({\n",
    "        'PULocationID': all_zones,\n",
    "        'pickup_weekday': [pickup_weekday] * len(all_zones),\n",
    "        'pickup_hour': [pickup_hour] * len(all_zones)\n",
    "    })\n",
    "    \n",
    "    # Realizar predicciones para todas las zonas y devolver los resultados individuales\n",
    "    predictions = model.predict(input_data)\n",
    "    result = {zone: prediction for zone, prediction in zip(all_zones, predictions)}\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Ejemplo de uso de la función predict_demand_with_adjacent\n",
    "predicted_demands = predict_demand_with_adjacent(26, 3, 19)  # 260 es el PULocationID, 3 es miércoles, 13 es la hora\n",
    "\n",
    "# Encontrar la zona con mayor demanda\n",
    "max_zone = max(predicted_demands, key=predicted_demands.get)\n",
    "print(f\"Zone with highest demand: {max_zone}, Demand: {predicted_demands[max_zone]:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crear Archivo de Machine Learning PLK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 3.67\n",
      "R²: 0.03\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import ast\n",
    "import joblib  # Importar joblib para guardar y cargar el modelo\n",
    "\n",
    "# 🌟 Cargar variables de entorno\n",
    "load_dotenv()\n",
    "\n",
    "# 🔐 Configuración de la conexión a la base de datos\n",
    "DB_CONFIG = {\n",
    "    'host': os.getenv(\"DB_HOST\"),\n",
    "    'port': int(os.getenv(\"DB_PORT\", 3306)),\n",
    "    'user': os.getenv(\"DB_USER\"),\n",
    "    'password': os.getenv(\"DB_PASSWORD\"),\n",
    "    'database': os.getenv(\"DB_NAME\")\n",
    "}\n",
    "\n",
    "# Crear el motor de SQLAlchemy\n",
    "engine = create_engine(\n",
    "    f\"mysql+pymysql://{DB_CONFIG['user']}:{DB_CONFIG['password']}@{DB_CONFIG['host']}:{DB_CONFIG['port']}/{DB_CONFIG['database']}\"\n",
    ")\n",
    "\n",
    "# Leer los datos desde la tabla enriched_taxi_data2\n",
    "query = \"SELECT * FROM enriched_taxi_data2\"\n",
    "df = pd.read_sql(query, engine)\n",
    "\n",
    "# Leer el archivo de zonas adyacentes\n",
    "adjacent_zones_df = pd.read_csv('adjacent_zones.csv')\n",
    "adjacent_zones_df['adjacent_zones'] = adjacent_zones_df['adjacent_zones'].apply(ast.literal_eval)\n",
    "\n",
    "# Función para obtener las zonas adyacentes\n",
    "def get_adjacent_zones(location_id):\n",
    "    row = adjacent_zones_df.loc[adjacent_zones_df['LocationID'] == location_id]\n",
    "    if not row.empty:\n",
    "        return row.iloc[0]['adjacent_zones']\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "# Seleccionar las características y la variable objetivo\n",
    "X = df[['PULocationID', 'pickup_weekday', 'pickup_hour']]\n",
    "y = df['trip_count']\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Crear y entrenar el modelo\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Guardar el modelo entrenado en un archivo\n",
    "joblib.dump(model, 'linear_regression_model.pkl')\n",
    "\n",
    "# Evaluar el rendimiento del modelo\n",
    "y_pred = model.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"MAE: {mae:.2f}\")\n",
    "print(f\"R²: {r2:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba modelo 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# 🌟 Cargar variables de entorno\n",
    "load_dotenv()\n",
    "\n",
    "# 🔐 Configuración de la conexión a la base de datos\n",
    "DB_CONFIG = {\n",
    "    'host': os.getenv(\"DB_HOST\"),\n",
    "    'port': int(os.getenv(\"DB_PORT\", 3306)),\n",
    "    'user': os.getenv(\"DB_USER\"),\n",
    "    'password': os.getenv(\"DB_PASSWORD\"),\n",
    "    'database': os.getenv(\"DB_NAME\")\n",
    "}\n",
    "\n",
    "# Crear el motor de SQLAlchemy\n",
    "engine = create_engine(\n",
    "    f\"mysql+pymysql://{DB_CONFIG['user']}:{DB_CONFIG['password']}@{DB_CONFIG['host']}:{DB_CONFIG['port']}/{DB_CONFIG['database']}\"\n",
    ")\n",
    "\n",
    "# Leer los datos desde la tabla taxi_fhv_data\n",
    "query = \"SELECT * FROM taxi_fhv_data\"\n",
    "df = pd.read_sql(query, engine)\n",
    "\n",
    "# Extraer información temporal\n",
    "df['pickup_weekday'] = pd.to_datetime(df['Pickup_datetime']).dt.dayofweek + 1  # De 1 (lunes) a 7 (domingo)\n",
    "df['pickup_hour'] = pd.to_datetime(df['Pickup_datetime']).dt.hour\n",
    "\n",
    "# Seleccionar las características y la variable objetivo\n",
    "X = df[['PULocationID', 'pickup_weekday', 'pickup_hour']]\n",
    "y = df['trip_count']\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Crear y entrenar el modelo\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Realizar predicciones sobre el conjunto de prueba\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluar el rendimiento del modelo\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"MAE: {mae:.2f}\")\n",
    "print(f\"R²: {r2:.2f}\")\n",
    "\n",
    "# Función para predecir la demanda dada una zona, día y hora\n",
    "def predict_demand(pulocationid, pickup_weekday, pickup_hour):\n",
    "    # Crear un DataFrame con las mismas columnas que el DataFrame original utilizado para entrenar el modelo\n",
    "    input_data = pd.DataFrame({\n",
    "        'PULocationID': [pulocationid],\n",
    "        'pickup_weekday': [pickup_weekday],\n",
    "        'pickup_hour': [pickup_hour]\n",
    "    })\n",
    "    \n",
    "    prediction = model.predict(input_data)\n",
    "    return prediction[0]\n",
    "\n",
    "# Ejemplo de uso de la función predict_demand\n",
    "predicted_demand = predict_demand(84, 3, 12)  # 84 es el PULocationID, 3 es miércoles, 12 es la hora\n",
    "print(f\"Predicted Demand: {predicted_demand:.2f}\")\n",
    "\n",
    "predicted_demand1 = predict_demand(99, 3, 12)  # 99 es el PULocationID, 3 es miércoles, 12 es la hora\n",
    "print(f\"Predicted Demand: {predicted_demand1:.2f}\")\n",
    "\n",
    "predicted_demand2 = predict_demand(204, 3, 12)  # 204 es el PULocationID, 3 es miércoles, 12 es la hora\n",
    "print(f\"Predicted Demand: {predicted_demand2:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
