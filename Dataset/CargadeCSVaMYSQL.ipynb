{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script de Importación de Trafico de NYC a MySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import mysql.connector\n",
    "from datetime import datetime\n",
    "\n",
    "# Cargar las variables de entorno desde el archivo .env\n",
    "load_dotenv()\n",
    "\n",
    "# Obtener las variables de entorno\n",
    "db_host = os.getenv('DB_HOST')\n",
    "db_user = os.getenv('DB_USER')\n",
    "db_password = os.getenv('DB_PASSWORD')\n",
    "db_name = os.getenv('DB_NAME')\n",
    "db_port = os.getenv('DB_PORT')\n",
    "csv_file_path = 'Automated_Traffic_Volume_Counts.csv'  # Asegúrate de actualizar esto con tu archivo CSV correcto\n",
    "\n",
    "# Verificar si las variables de entorno se cargaron correctamente\n",
    "if not all([db_host, db_user, db_password, db_name, db_port]):\n",
    "    raise ValueError(\"No se pudieron cargar todas las variables de entorno. Por favor verifica el archivo .env.\")\n",
    "\n",
    "# Cargar el archivo CSV en un DataFrame de pandas\n",
    "try:\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "except FileNotFoundError as e:\n",
    "    raise FileNotFoundError(f\"El archivo {csv_file_path} no se encuentra. Por favor verifica la ruta.\")\n",
    "\n",
    "# Filtrar los datos desde enero 2023 hasta agosto 2024\n",
    "start_date = datetime(2023, 1, 1)\n",
    "end_date = datetime(2024, 8, 31)\n",
    "\n",
    "# Ensure the columns are in the correct format\n",
    "df['Yr'] = df['Yr'].astype(int)\n",
    "df['M'] = df['M'].astype(int)\n",
    "df['D'] = df['D'].astype(int)\n",
    "df['HH'] = df['HH'].astype(int)\n",
    "df['MM'] = df['MM'].astype(int)\n",
    "\n",
    "# Create a datetime column\n",
    "df['datetime'] = pd.to_datetime(df[['Yr', 'M', 'D', 'HH', 'MM']].rename(columns={'Yr': 'year', 'M': 'month', 'D': 'day', 'HH': 'hour', 'MM': 'minute'}))\n",
    "\n",
    "df_filtered = df[(df['datetime'] >= start_date) & (df['datetime'] <= end_date)].copy()\n",
    "\n",
    "# Seleccionar solo las columnas necesarias para el análisis\n",
    "columns_to_keep = ['RequestID', 'Boro', 'Yr', 'M', 'D', 'HH', 'MM', 'Vol', 'SegmentID', 'WktGeom', 'street', 'Direction']\n",
    "df_filtered = df_filtered[columns_to_keep]\n",
    "\n",
    "# Conectar a la base de datos MySQL\n",
    "try:\n",
    "    db_connection = mysql.connector.connect(\n",
    "        host=db_host,\n",
    "        user=db_user,\n",
    "        password=db_password,\n",
    "        database=db_name,\n",
    "        port=int(db_port)\n",
    "    )\n",
    "except mysql.connector.Error as err:\n",
    "    raise mysql.connector.Error(f\"Error al conectar a la base de datos: {err}\")\n",
    "\n",
    "cursor = db_connection.cursor()\n",
    "\n",
    "# Insertar los datos filtrados del DataFrame en bloques de 100,000 filas\n",
    "chunk_size = 100000\n",
    "rows_inserted = 0\n",
    "for start in range(0, len(df_filtered), chunk_size):\n",
    "    end = start + chunk_size\n",
    "    chunk = df_filtered.iloc[start:end]\n",
    "\n",
    "    insert_query = \"\"\"\n",
    "    INSERT INTO trafico (RequestID, Boro, Yr, M, D, HH, MM, Vol, SegmentID, WktGeom, Direction)\n",
    "    VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "    \"\"\"\n",
    "    data = [(\n",
    "        row['RequestID'], row['Boro'], row['Yr'], row['M'], row['D'], row['HH'], row['MM'], row['Vol'],\n",
    "        row['SegmentID'], row['WktGeom'], row['Direction']\n",
    "    ) for _, row in chunk.iterrows()]\n",
    "\n",
    "    cursor.executemany(insert_query, data)\n",
    "    rows_inserted += len(data)\n",
    "    \n",
    "    # Imprimir el progreso solo cada 10 bloques\n",
    "    if start // chunk_size % 10 == 0:\n",
    "        print(f\"Se han insertado {len(data)} filas en este bloque. Total filas insertadas: {rows_inserted}\")\n",
    "\n",
    "# Confirmar los cambios y cerrar la conexión\n",
    "db_connection.commit()\n",
    "cursor.close()\n",
    "db_connection.close()\n",
    "\n",
    "# Mostrar el número total de filas insertadas\n",
    "print(f\"Se han insertado un total de {rows_inserted} filas en la base de datos.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script de Importación de Temperaturas Promedio de NYC a MySQL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se han insertado 12 filas en la base de datos.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import mysql.connector\n",
    "\n",
    "# Cargar las variables de entorno desde el archivo .env\n",
    "load_dotenv()\n",
    "\n",
    "# Obtener las variables de entorno\n",
    "db_host = os.getenv('DB_HOST')\n",
    "db_user = os.getenv('DB_USER')\n",
    "db_password = os.getenv('DB_PASSWORD')\n",
    "db_name = os.getenv('DB_NAME')\n",
    "db_port = os.getenv('DB_PORT')\n",
    "csv_file_path = 'temperaturas_promedio_nyc_mensual.csv'  # Asegúrate de actualizar esto con tu archivo CSV correcto\n",
    "\n",
    "# Verificar si las variables de entorno se cargaron correctamente\n",
    "if not all([db_host, db_user, db_password, db_name, db_port]):\n",
    "    raise ValueError(\"No se pudieron cargar todas las variables de entorno. Por favor verifica el archivo .env.\")\n",
    "\n",
    "# Cargar el archivo CSV en un DataFrame de pandas\n",
    "try:\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "except FileNotFoundError as e:\n",
    "    raise FileNotFoundError(f\"El archivo {csv_file_path} no se encuentra. Por favor verifica la ruta.\")\n",
    "\n",
    "# Conectar a la base de datos MySQL\n",
    "try:\n",
    "    db_connection = mysql.connector.connect(\n",
    "        host=db_host,\n",
    "        user=db_user,\n",
    "        password=db_password,\n",
    "        database=db_name,\n",
    "        port=int(db_port)\n",
    "    )\n",
    "except mysql.connector.Error as err:\n",
    "    raise mysql.connector.Error(f\"Error al conectar a la base de datos: {err}\")\n",
    "\n",
    "cursor = db_connection.cursor()\n",
    "\n",
    "# Insertar los datos del DataFrame en la tabla MySQL\n",
    "insert_query = \"\"\"\n",
    "INSERT INTO temperaturas (Mes, Manhattan, Brooklyn, Queens, The_Bronx, Staten_Island)\n",
    "VALUES (%s, %s, %s, %s, %s, %s)\n",
    "\"\"\"\n",
    "data = [tuple(row) for row in df.values]\n",
    "\n",
    "cursor.executemany(insert_query, data)\n",
    "\n",
    "# Confirmar los cambios y cerrar la conexión\n",
    "db_connection.commit()\n",
    "cursor.close()\n",
    "db_connection.close()\n",
    "\n",
    "# Mostrar el número de filas insertadas\n",
    "print(f\"Se han insertado {len(data)} filas en la base de datos.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script de Importación de Taxi_Zones Promedio de NYC a MySQL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import mysql.connector\n",
    "\n",
    "# Cargar las variables de entorno desde el archivo .env\n",
    "print(\"Cargando las variables de entorno...\")\n",
    "load_dotenv()\n",
    "\n",
    "# Obtener las variables de entorno\n",
    "db_host = os.getenv('DB_HOST')\n",
    "db_user = os.getenv('DB_USER')\n",
    "db_password = os.getenv('DB_PASSWORD')\n",
    "db_name = os.getenv('DB_NAME')\n",
    "db_port = os.getenv('DB_PORT')\n",
    "csv_file_path = 'taxi_zones.csv'  # Asegúrate de actualizar esto con tu archivo CSV correcto\n",
    "\n",
    "# Verificar si las variables de entorno se cargaron correctamente\n",
    "if not all([db_host, db_user, db_password, db_name, db_port]):\n",
    "    raise ValueError(\"No se pudieron cargar todas las variables de entorno. Por favor verifica el archivo .env.\")\n",
    "print(\"Variables de entorno cargadas exitosamente.\")\n",
    "\n",
    "# Cargar el archivo CSV en un DataFrame de pandas\n",
    "print(f\"Cargando el archivo CSV desde {csv_file_path}...\")\n",
    "try:\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "except FileNotFoundError as e:\n",
    "    raise FileNotFoundError(f\"El archivo {csv_file_path} no se encuentra. Por favor verifica la ruta.\")\n",
    "print(\"Archivo CSV cargado exitosamente.\")\n",
    "\n",
    "# Verificar los nombres de las columnas\n",
    "print(\"Verificando los nombres de las columnas...\")\n",
    "print(df.columns)\n",
    "\n",
    "# Conectar a la base de datos MySQL\n",
    "print(\"Conectando a la base de datos MySQL...\")\n",
    "try:\n",
    "    db_connection = mysql.connector.connect(\n",
    "        host=db_host,\n",
    "        user=db_user,\n",
    "        password=db_password,\n",
    "        database=db_name,\n",
    "        port=int(db_port)\n",
    "    )\n",
    "except mysql.connector.Error as err:\n",
    "    raise mysql.connector.Error(f\"Error al conectar a la base de datos: {err}\")\n",
    "print(\"Conexión a la base de datos exitosa.\")\n",
    "\n",
    "cursor = db_connection.cursor()\n",
    "\n",
    "# Insertar los datos del DataFrame en la tabla MySQL\n",
    "print(\"Insertando datos en la tabla taxi_zones en la base de datos...\")\n",
    "insert_query = \"\"\"\n",
    "INSERT INTO taxi_zones (LocationID, Borough, Zone, service_zone)\n",
    "VALUES (%s, %s, %s, %s)\n",
    "\"\"\"\n",
    "data = [(\n",
    "    row['LocationID'], row['Borough'], row['Zone'], row['service_zone']\n",
    ") for _, row in df.iterrows()]\n",
    "\n",
    "cursor.executemany(insert_query, data)\n",
    "\n",
    "# Confirmar los cambios y cerrar la conexión\n",
    "print(\"Confirmando los cambios y cerrando la conexión a la base de datos...\")\n",
    "db_connection.commit()\n",
    "cursor.close()\n",
    "db_connection.close()\n",
    "print(\"Conexión cerrada.\")\n",
    "\n",
    "# Mostrar el número total de filas insertadas\n",
    "print(f\"Se han insertado un total de {len(data)} filas en la tabla taxi_zones en la base de datos.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script de Importación de Trafico de NYC a MySQL convirtiendo geometrías WKT a objetos Shapely\n",
    "\n",
    "se requiere libreria geopandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 21:54:32,608 - INFO - Starting ETL process for dates 2024-01-01 to 2024-01-31\n",
      "2024-11-18 21:54:36,880 - INFO - Retrieved 0 temperature records\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import logging\n",
    "from sqlalchemy import create_engine, text\n",
    "from datetime import datetime\n",
    "from typing import Optional, Dict, Any\n",
    "import numpy as np\n",
    "\n",
    "class NYCTaxiTemperatureETL:\n",
    "    def __init__(self, user: str, password: str, host: str, database: str = 'UrbanTransit'):\n",
    "        \"\"\"Initialize ETL process with database credentials.\"\"\"\n",
    "        self.connection_string = f'mysql+mysqlconnector://{user}:{password}@{host}/{database}'\n",
    "        \n",
    "        # Configure logging\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        if not self.logger.handlers:\n",
    "            handler = logging.StreamHandler()\n",
    "            formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "            handler.setFormatter(formatter)\n",
    "            self.logger.addHandler(handler)\n",
    "            self.logger.setLevel(logging.INFO)\n",
    "        \n",
    "        # Initialize lookup tables\n",
    "        self.borough_zones = self._initialize_zone_mappings()\n",
    "\n",
    "    def create_engine(self):\n",
    "        \"\"\"Create and return a database engine with connection pooling.\"\"\"\n",
    "        return create_engine(\n",
    "            self.connection_string,\n",
    "            pool_recycle=3600,\n",
    "            pool_pre_ping=True,\n",
    "            pool_size=5,\n",
    "            max_overflow=10\n",
    "        )\n",
    "\n",
    "    def _initialize_zone_mappings(self) -> Dict[int, str]:\n",
    "        \"\"\"Initialize mapping between taxi zones and boroughs.\"\"\"\n",
    "        # This would ideally come from a database table, but for now we'll hardcode some examples\n",
    "        return {\n",
    "            1: \"Manhattan\",\n",
    "            2: \"Brooklyn\",\n",
    "            3: \"Queens\",\n",
    "            4: \"The_Bronx\",\n",
    "            5: \"Staten_Island\"\n",
    "        }\n",
    "\n",
    "    def _get_borough_from_location(self, location_id: int) -> str:\n",
    "        \"\"\"Convert location ID to borough name.\"\"\"\n",
    "        return self.borough_zones.get(location_id, \"Unknown\")\n",
    "\n",
    "    def process_taxi_chunk(self, chunk: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Process a chunk of taxi data.\"\"\"\n",
    "        # Convert datetime columns\n",
    "        for col in ['Pickup_datetime', 'DropOff_datetime']:\n",
    "            if col in chunk.columns:\n",
    "                chunk[col] = pd.to_datetime(chunk[col])\n",
    "        \n",
    "        # Extract datetime components for joining with temperature data\n",
    "        chunk['pickup_year'] = chunk['Pickup_datetime'].dt.year\n",
    "        chunk['pickup_month'] = chunk['Pickup_datetime'].dt.month\n",
    "        chunk['pickup_day'] = chunk['Pickup_datetime'].dt.day\n",
    "        chunk['pickup_hour'] = chunk['Pickup_datetime'].dt.hour\n",
    "        chunk['pickup_minute'] = chunk['Pickup_datetime'].dt.minute\n",
    "        \n",
    "        # Get borough information\n",
    "        chunk['pickup_borough'] = chunk['PULocationID'].map(self._get_borough_from_location)\n",
    "        \n",
    "        return chunk\n",
    "\n",
    "    def get_temperature_data(self, start_date: str, end_date: str) -> pd.DataFrame:\n",
    "        \"\"\"Get temperature data for the specified date range.\"\"\"\n",
    "        query = \"\"\"\n",
    "        SELECT \n",
    "            Mes,\n",
    "            Manhattan, Brooklyn, Queens, The_Bronx, Staten_Island\n",
    "        FROM temperaturas\n",
    "        WHERE DATE_FORMAT(STR_TO_DATE(CONCAT('2024-', Mes, '-01'), '%Y-%m-%d'), '%Y-%m') BETWEEN DATE_FORMAT(:start_date, '%Y-%m') AND DATE_FORMAT(:end_date, '%Y-%m')\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            engine = self.create_engine()\n",
    "            with engine.connect() as conn:\n",
    "                temperature_data = pd.read_sql_query(\n",
    "                    text(query),\n",
    "                    con=conn,\n",
    "                    params={'start_date': start_date, 'end_date': end_date}\n",
    "                )\n",
    "            return temperature_data\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error fetching temperature data: {str(e)}\")\n",
    "            raise\n",
    "        finally:\n",
    "            engine.dispose()\n",
    "\n",
    "    def enrich_with_temperature(self, taxi_chunk: pd.DataFrame, temperature_data: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Enrich taxi data with temperature information.\"\"\"\n",
    "        # Create month key for joining\n",
    "        taxi_chunk['pickup_month'] = taxi_chunk['Pickup_datetime'].dt.month\n",
    "        \n",
    "        # Merge taxi data with temperature data\n",
    "        merged = pd.merge(\n",
    "            taxi_chunk,\n",
    "            temperature_data,\n",
    "            left_on='pickup_month',\n",
    "            right_on='Mes',\n",
    "            how='left'\n",
    "        )\n",
    "        \n",
    "        # Get temperature based on pickup borough\n",
    "        for borough in ['Manhattan', 'Brooklyn', 'Queens', 'The_Bronx', 'Staten_Island']:\n",
    "            merged[borough + '_temperature'] = merged.apply(\n",
    "                lambda row: row[borough] if row['pickup_borough'] == borough else None,\n",
    "                axis=1\n",
    "            )\n",
    "        \n",
    "        # Select relevant columns\n",
    "        merged['temperature'] = merged[['Manhattan_temperature', 'Brooklyn_temperature', 'Queens_temperature', 'The_Bronx_temperature', 'Staten_Island_temperature']].bfill(axis=1).iloc[:, 0]\n",
    "        \n",
    "        return merged\n",
    "\n",
    "    def save_enriched_data(self, enriched_data: pd.DataFrame) -> None:\n",
    "        \"\"\"Save enriched data to the database.\"\"\"\n",
    "        insert_query = \"\"\"\n",
    "        INSERT INTO enriched_taxi_data (\n",
    "            Pickup_datetime, DropOff_datetime, \n",
    "            PULocationID, DOLocationID,\n",
    "            pickup_borough, temperature,\n",
    "            pickup_year, pickup_month, pickup_day, pickup_hour, pickup_minute\n",
    "        ) VALUES (\n",
    "            %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s\n",
    "        )\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            engine = self.create_engine()\n",
    "            with engine.connect() as conn:\n",
    "                for _, row in enriched_data.iterrows():\n",
    "                    conn.execute(text(insert_query), (\n",
    "                        row['Pickup_datetime'],\n",
    "                        row['DropOff_datetime'],\n",
    "                        row['PULocationID'],\n",
    "                        row['DOLocationID'],\n",
    "                        row['pickup_borough'],\n",
    "                        row['temperature'],\n",
    "                        row['pickup_year'],\n",
    "                        row['pickup_month'],\n",
    "                        row['pickup_day'],\n",
    "                        row['pickup_hour'],\n",
    "                        row['pickup_minute']\n",
    "                    ))\n",
    "                conn.commit()\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error saving enriched data: {str(e)}\")\n",
    "            raise\n",
    "        finally:\n",
    "            engine.dispose()\n",
    "\n",
    "    def run_etl_process(self, start_date: str, end_date: str, chunksize: int = 100000) -> None:\n",
    "        \"\"\"Run the complete ETL process.\"\"\"\n",
    "        self.logger.info(f\"Starting ETL process for dates {start_date} to {end_date}\")\n",
    "        engine = None\n",
    "\n",
    "        try:\n",
    "            # Get temperature data for the entire period\n",
    "            temperature_data = self.get_temperature_data(start_date, end_date)\n",
    "            self.logger.info(f\"Retrieved {len(temperature_data)} temperature records\")\n",
    "            \n",
    "            # Process taxi data in chunks\n",
    "            offset = 0\n",
    "            total_inserted_rows = 0\n",
    "            \n",
    "            while True:\n",
    "                # Get taxi data chunk\n",
    "                query = f\"\"\"\n",
    "                SELECT * FROM taxi_fhv_data \n",
    "                WHERE Pickup_datetime BETWEEN :start_date AND :end_date\n",
    "                LIMIT {chunksize} OFFSET {offset}\n",
    "                \"\"\"\n",
    "                \n",
    "                engine = self.create_engine()\n",
    "                with engine.connect() as conn:\n",
    "                    taxi_chunk = pd.read_sql_query(\n",
    "                        text(query),\n",
    "                        con=conn,\n",
    "                        params={'start_date': start_date, 'end_date': end_date}\n",
    "                    )\n",
    "                \n",
    "                if taxi_chunk.empty:\n",
    "                    self.logger.info(\"No more taxi data to process\")\n",
    "                    break\n",
    "                \n",
    "                self.logger.info(f\"Processing chunk of {len(taxi_chunk)} records\")\n",
    "                \n",
    "                # Process the chunk\n",
    "                processed_chunk = self.process_taxi_chunk(taxi_chunk)\n",
    "                enriched_chunk = self.enrich_with_temperature(processed_chunk, temperature_data)\n",
    "                \n",
    "                # Save the enriched data\n",
    "                self.save_enriched_data(enriched_chunk)\n",
    "                \n",
    "                inserted_rows = len(enriched_chunk)\n",
    "                total_inserted_rows += inserted_rows\n",
    "                \n",
    "                self.logger.info(f\"Completed processing chunk, offset: {offset}\")\n",
    "                offset += chunksize\n",
    "            \n",
    "            self.logger.info(f\"Total rows inserted: {total_inserted_rows}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error in ETL process: {str(e)}\")\n",
    "            raise\n",
    "        finally:\n",
    "            if engine:\n",
    "                engine.dispose()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize ETL process\n",
    "    etl = NYCTaxiTemperatureETL(\n",
    "        user='usuario_admin',\n",
    "        password='AWSSIEMPRE__',\n",
    "        host='urbantransit-db2.cfou8mqoatn0.us-east-2.rds.amazonaws.com'\n",
    "    )\n",
    "    \n",
    "    # Run ETL process for a specific date range\n",
    "    etl.run_etl_process(\n",
    "        start_date='2024-01-01',\n",
    "        end_date='2024-01-31',\n",
    "        chunksize=100000\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
