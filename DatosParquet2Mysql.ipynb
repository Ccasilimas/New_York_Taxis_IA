{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conexión a la base de datos establecida con éxito.\n",
      "\n",
      "Leyendo archivo D:\\GitHub\\Proyecto Final\\Proyecto_final_Henry\\Dataset\\yellow_tripdata_2023-12.parquet\n",
      "El archivo yellow_tripdata_2023-12.parquet contiene todas las columnas necesarias. Procesando...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando yellow_tripdata_2023-12.parquet:   4%|▍         | 1/23 [00:22<08:07, 22.16s/chunk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/23 del archivo yellow_tripdata_2023-12.parquet cargado exitosamente.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando yellow_tripdata_2023-12.parquet:   9%|▊         | 2/23 [00:37<06:17, 17.99s/chunk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 2/23 del archivo yellow_tripdata_2023-12.parquet cargado exitosamente.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando yellow_tripdata_2023-12.parquet:  13%|█▎        | 3/23 [00:52<05:37, 16.88s/chunk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 3/23 del archivo yellow_tripdata_2023-12.parquet cargado exitosamente.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando yellow_tripdata_2023-12.parquet:  17%|█▋        | 4/23 [01:07<05:05, 16.09s/chunk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 4/23 del archivo yellow_tripdata_2023-12.parquet cargado exitosamente.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando yellow_tripdata_2023-12.parquet:  22%|██▏       | 5/23 [01:23<04:49, 16.11s/chunk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 5/23 del archivo yellow_tripdata_2023-12.parquet cargado exitosamente.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando yellow_tripdata_2023-12.parquet:  26%|██▌       | 6/23 [01:41<04:40, 16.52s/chunk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 6/23 del archivo yellow_tripdata_2023-12.parquet cargado exitosamente.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando yellow_tripdata_2023-12.parquet:  30%|███       | 7/23 [01:58<04:29, 16.84s/chunk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 7/23 del archivo yellow_tripdata_2023-12.parquet cargado exitosamente.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando yellow_tripdata_2023-12.parquet:  35%|███▍      | 8/23 [02:16<04:15, 17.03s/chunk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 8/23 del archivo yellow_tripdata_2023-12.parquet cargado exitosamente.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando yellow_tripdata_2023-12.parquet:  39%|███▉      | 9/23 [02:34<04:02, 17.33s/chunk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 9/23 del archivo yellow_tripdata_2023-12.parquet cargado exitosamente.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando yellow_tripdata_2023-12.parquet:  43%|████▎     | 10/23 [02:52<03:49, 17.62s/chunk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 10/23 del archivo yellow_tripdata_2023-12.parquet cargado exitosamente.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando yellow_tripdata_2023-12.parquet:  48%|████▊     | 11/23 [03:11<03:37, 18.15s/chunk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 11/23 del archivo yellow_tripdata_2023-12.parquet cargado exitosamente.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando yellow_tripdata_2023-12.parquet:  52%|█████▏    | 12/23 [03:30<03:22, 18.37s/chunk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 12/23 del archivo yellow_tripdata_2023-12.parquet cargado exitosamente.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando yellow_tripdata_2023-12.parquet:  57%|█████▋    | 13/23 [03:47<02:58, 17.81s/chunk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 13/23 del archivo yellow_tripdata_2023-12.parquet cargado exitosamente.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando yellow_tripdata_2023-12.parquet:  61%|██████    | 14/23 [04:02<02:32, 16.95s/chunk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 14/23 del archivo yellow_tripdata_2023-12.parquet cargado exitosamente.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando yellow_tripdata_2023-12.parquet:  65%|██████▌   | 15/23 [04:17<02:12, 16.54s/chunk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 15/23 del archivo yellow_tripdata_2023-12.parquet cargado exitosamente.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando yellow_tripdata_2023-12.parquet:  70%|██████▉   | 16/23 [04:35<01:58, 16.90s/chunk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 16/23 del archivo yellow_tripdata_2023-12.parquet cargado exitosamente.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando yellow_tripdata_2023-12.parquet:  74%|███████▍  | 17/23 [04:54<01:45, 17.58s/chunk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 17/23 del archivo yellow_tripdata_2023-12.parquet cargado exitosamente.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando yellow_tripdata_2023-12.parquet:  78%|███████▊  | 18/23 [05:10<01:26, 17.21s/chunk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 18/23 del archivo yellow_tripdata_2023-12.parquet cargado exitosamente.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando yellow_tripdata_2023-12.parquet:  83%|████████▎ | 19/23 [05:26<01:07, 16.84s/chunk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 19/23 del archivo yellow_tripdata_2023-12.parquet cargado exitosamente.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando yellow_tripdata_2023-12.parquet:  87%|████████▋ | 20/23 [05:43<00:50, 16.70s/chunk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 20/23 del archivo yellow_tripdata_2023-12.parquet cargado exitosamente.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando yellow_tripdata_2023-12.parquet:  91%|█████████▏| 21/23 [06:02<00:35, 17.56s/chunk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 21/23 del archivo yellow_tripdata_2023-12.parquet cargado exitosamente.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando yellow_tripdata_2023-12.parquet:  96%|█████████▌| 22/23 [06:19<00:17, 17.25s/chunk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 22/23 del archivo yellow_tripdata_2023-12.parquet cargado exitosamente.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando yellow_tripdata_2023-12.parquet: 100%|██████████| 23/23 [06:28<00:00, 16.87s/chunk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 23/23 del archivo yellow_tripdata_2023-12.parquet cargado exitosamente.\n",
      "Datos del archivo yellow_tripdata_2023-12.parquet cargados exitosamente.\n",
      "Error al leer D:\\GitHub\\Proyecto Final\\Proyecto_final_Henry\\Dataset\\fhv_tripdata_2024-01.parquet: [Errno 2] No such file or directory: 'D:\\\\GitHub\\\\Proyecto Final\\\\Proyecto_final_Henry\\\\Dataset\\\\fhv_tripdata_2024-01.parquet'\n",
      "\n",
      "Proceso de estandarización y carga completo.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sqlalchemy\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "import time  # Importar el módulo time para pausar\n",
    "\n",
    "# Cargar las variables de entorno desde el archivo .env\n",
    "load_dotenv()\n",
    "\n",
    "# Obtener las credenciales de la base de datos desde las variables de entorno\n",
    "db_user = os.getenv('DB_USER')\n",
    "db_password = os.getenv('DB_PASSWORD')\n",
    "db_host = os.getenv('DB_HOST')\n",
    "db_port = os.getenv('DB_PORT')\n",
    "db_name = os.getenv('DB_NAME')\n",
    "\n",
    "# Función para conectar a la base de datos\n",
    "def connect_to_db():\n",
    "    try:\n",
    "        engine = sqlalchemy.create_engine(\n",
    "            f'mysql+pymysql://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}'\n",
    "        )\n",
    "        connection = engine.connect()\n",
    "        print(\"Conexión a la base de datos establecida con éxito.\")\n",
    "        return engine, connection\n",
    "    except Exception as e:\n",
    "        print(f\"Error al conectar a la base de datos: {e}\")\n",
    "        raise\n",
    "\n",
    "# Conectar a MySQL utilizando las credenciales del archivo .env\n",
    "engine, connection = connect_to_db()\n",
    "\n",
    "# Ruta de la carpeta con los archivos\n",
    "folder_path = 'D:\\\\GitHub\\\\Proyecto Final\\\\Proyecto_final_Henry\\\\Dataset'\n",
    "\n",
    "# Columnas a mantener para green_tripdata\n",
    "columns_to_keep_green = [\n",
    "    'lpep_pickup_datetime', 'lpep_dropoff_datetime',\n",
    "    'PULocationID', 'DOLocationID',\n",
    "    'trip_distance', 'total_amount'\n",
    "]\n",
    "\n",
    "# Columnas a mantener para yellow_tripdata\n",
    "columns_to_keep_yellow = [\n",
    "    'tpep_pickup_datetime', 'tpep_dropoff_datetime',\n",
    "    'PULocationID', 'DOLocationID',\n",
    "    'trip_distance', 'total_amount'\n",
    "]\n",
    "\n",
    "# Columnas a mantener para fhv_tripdata\n",
    "columns_to_keep_fhv = [\n",
    "    'pickup_datetime', 'dropOff_datetime',\n",
    "    'PUlocationID', 'DOlocationID',\n",
    "    'Affiliated_base_number'\n",
    "]\n",
    "\n",
    "# Leer todos los archivos en la carpeta y estandarizar las columnas\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.parquet'):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "        try:\n",
    "            # Leer archivo .parquet\n",
    "            print(f\"\\nLeyendo archivo {file_path}\")\n",
    "            df = pd.read_parquet(file_path)\n",
    "\n",
    "            # Verificar si es green_tripdata, yellow_tripdata o fhv_tripdata\n",
    "            if 'green' in filename.lower():\n",
    "                columns_to_keep = columns_to_keep_green\n",
    "                source_value = 'green_tripdata'\n",
    "            elif 'yellow' in filename.lower():\n",
    "                columns_to_keep = columns_to_keep_yellow\n",
    "                source_value = 'yellow_tripdata'\n",
    "            elif 'fhv' in filename.lower():\n",
    "                columns_to_keep = columns_to_keep_fhv\n",
    "                source_value = 'fhv_tripdata'\n",
    "            else:\n",
    "                print(f\"El archivo {filename} no es ni 'green', 'yellow' ni 'fhv'. Saltando.\")\n",
    "                continue\n",
    "\n",
    "            # Verificar si el archivo contiene todas las columnas necesarias\n",
    "            if all(column in df.columns for column in columns_to_keep):\n",
    "                print(f\"El archivo {filename} contiene todas las columnas necesarias. Procesando...\")\n",
    "\n",
    "                # Seleccionar columnas necesarias\n",
    "                df = df[columns_to_keep]\n",
    "\n",
    "                # Renombrar columnas según sea necesario\n",
    "                if 'yellow' in filename.lower():\n",
    "                    df.rename(columns={\n",
    "                        'tpep_pickup_datetime': 'Pickup_datetime',\n",
    "                        'tpep_dropoff_datetime': 'DropOff_datetime',\n",
    "                        'trip_distance': 'trip_miles',\n",
    "                        'total_amount': 'driver_pay'\n",
    "                    }, inplace=True)\n",
    "                elif 'green' in filename.lower():\n",
    "                    df.rename(columns={\n",
    "                        'lpep_pickup_datetime': 'Pickup_datetime',\n",
    "                        'lpep_dropoff_datetime': 'DropOff_datetime',\n",
    "                        'trip_distance': 'trip_miles',\n",
    "                        'total_amount': 'driver_pay'\n",
    "                    }, inplace=True)\n",
    "                elif 'fhv' in filename.lower():\n",
    "                    df.rename(columns={\n",
    "                        'pickup_datetime': 'Pickup_datetime',\n",
    "                        'dropOff_datetime': 'DropOff_datetime',\n",
    "                        'PUlocationID': 'PULocationID',\n",
    "                        'DOlocationID': 'DOLocationID'\n",
    "                    }, inplace=True)\n",
    "                    df['trip_miles'] = 0.0  # Agregar columna trip_miles con valores por defecto\n",
    "                    df['driver_pay'] = 0.0  # Agregar columna driver_pay con valores por defecto\n",
    "\n",
    "                # Añadir la columna 'source'\n",
    "                df['source'] = source_value\n",
    "\n",
    "                # Calcular trip_time como la diferencia entre DropOff_datetime y Pickup_datetime\n",
    "                df['Pickup_datetime'] = pd.to_datetime(df['Pickup_datetime'])\n",
    "                df['DropOff_datetime'] = pd.to_datetime(df['DropOff_datetime'])\n",
    "                df['trip_time'] = (df['DropOff_datetime'] - df['Pickup_datetime']).dt.total_seconds()\n",
    "\n",
    "                # Dividir el DataFrame en bloques de 5,000 filas\n",
    "                chunk_size = 150000  # Tamaño del chunk a 5,000 filas\n",
    "                num_chunks = len(df) // chunk_size + 1\n",
    "                chunk_counter = 0  # Contador de chunks procesados\n",
    "\n",
    "                for i in tqdm(range(num_chunks), desc=f\"Procesando {filename}\", unit=\"chunk\"):\n",
    "                    chunk = df[i * chunk_size:(i + 1) * chunk_size]\n",
    "                    try:\n",
    "                        chunk.to_sql(name='taxi_fhv_data', con=engine, if_exists='append', index=False)\n",
    "                        print(f\"Chunk {i+1}/{num_chunks} del archivo {filename} cargado exitosamente.\")\n",
    "                    except sqlalchemy.exc.OperationalError as e:\n",
    "                        print(f\"Error al cargar chunk {i+1}: {e}. Intentando reconectar...\")\n",
    "                        engine, connection = connect_to_db()\n",
    "                        chunk.to_sql(name='taxi_fhv_data', con=engine, if_exists='append', index=False)\n",
    "                        print(f\"Chunk {i+1} del archivo {filename} cargado exitosamente tras reconexión.\")\n",
    "\n",
    "                    chunk_counter += 1\n",
    "\n",
    "                    # Cada 20 chunks, esperar 15 segundos y reconectar\n",
    "                    if chunk_counter % 30 == 0:\n",
    "                        print(\"Esperando 15 segundos...\")\n",
    "                        time.sleep(30)\n",
    "                        print(\"Reconectando a la base de datos...\")\n",
    "                        connection.close()\n",
    "                        engine.dispose()\n",
    "                        engine, connection = connect_to_db()\n",
    "                        print(\"Reconexión exitosa.\")\n",
    "\n",
    "                print(f\"Datos del archivo {filename} cargados exitosamente.\")\n",
    "            else:\n",
    "                print(f\"El archivo {filename} no contiene todas las columnas necesarias. Saltando.\")\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Archivo {file_path} no encontrado. Saltando.\")\n",
    "        except KeyError as e:\n",
    "            print(f\"Error de columna en {file_path}: {e}. Saltando.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error al procesar el archivo {filename}: {e}\")\n",
    "\n",
    "# Ahora, inspecciona el archivo específico fhv_tripdata_2024-01.parquet\n",
    "file_to_inspect = 'fhv_tripdata_2024-01.parquet'\n",
    "file_path = os.path.join(folder_path, file_to_inspect)\n",
    "\n",
    "try:\n",
    "    # Leer archivo .parquet\n",
    "    df = pd.read_parquet(file_path)\n",
    "\n",
    "    # Mostrar las columnas del archivo\n",
    "    print(f\"\\nColumnas del archivo {file_to_inspect}:\")\n",
    "    print(df.columns.tolist())\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error al leer {file_path}: {e}\")\n",
    "\n",
    "print(\"\\nProceso de estandarización y carga completo.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
