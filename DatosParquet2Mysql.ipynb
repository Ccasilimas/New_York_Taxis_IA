{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conexión a la base de datos establecida con éxito.\n",
      "Leyendo archivo D:\\GitHub\\Proyecto Final\\Proyecto_final_Henry\\Dataset\\fhv_tripdata_2024-01.parquet\n",
      "El archivo fhv_tripdata_2024-01.parquet contiene todas las columnas necesarias. Procesando...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando fhv_tripdata_2024-01.parquet:   0%|          | 0/130 [00:04<?, ?chunk/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error al procesar el archivo fhv_tripdata_2024-01.parquet: (pymysql.err.OperationalError) (1054, \"Unknown column 'Affiliated_base_number' in 'field list'\")\n",
      "[SQL: INSERT INTO taxi_fhv_data (`Pickup_datetime`, `DropOff_datetime`, `PULocationID`, `DOLocationID`, `Affiliated_base_number`, trip_miles, driver_pay, source, trip_time) VALUES (%(Pickup_datetime)s, %(DropOff_datetime)s, %(PULocationID)s, %(DOLocationID)s, %(Affiliated_base_number)s, %(trip_miles)s, %(driver_pay)s, %(source)s, %(trip_time)s)]\n",
      "[parameters: [{'Pickup_datetime': datetime.datetime(2024, 1, 1, 0, 15), 'DropOff_datetime': datetime.datetime(2024, 1, 1, 2, 13), 'PULocationID': None, 'DOLocationID': None, 'Affiliated_base_number': 'B00014', 'trip_miles': 0.0, 'driver_pay': 0.0, 'source': 'fhv_tripdata', 'trip_time': 7080.0}, {'Pickup_datetime': datetime.datetime(2024, 1, 1, 0, 30), 'DropOff_datetime': datetime.datetime(2024, 1, 1, 2, 37), 'PULocationID': None, 'DOLocationID': None, 'Affiliated_base_number': 'B00111', 'trip_miles': 0.0, 'driver_pay': 0.0, 'source': 'fhv_tripdata', 'trip_time': 7620.0}, {'Pickup_datetime': datetime.datetime(2024, 1, 1, 0, 27, 24), 'DropOff_datetime': datetime.datetime(2024, 1, 1, 1, 12, 5), 'PULocationID': None, 'DOLocationID': 14.0, 'Affiliated_base_number': 'B00112', 'trip_miles': 0.0, 'driver_pay': 0.0, 'source': 'fhv_tripdata', 'trip_time': 2681.0}, {'Pickup_datetime': datetime.datetime(2024, 1, 1, 0, 10, 9), 'DropOff_datetime': datetime.datetime(2024, 1, 1, 0, 25, 39), 'PULocationID': None, 'DOLocationID': 133.0, 'Affiliated_base_number': 'B00112', 'trip_miles': 0.0, 'driver_pay': 0.0, 'source': 'fhv_tripdata', 'trip_time': 930.0}, {'Pickup_datetime': datetime.datetime(2024, 1, 1, 0, 57, 7), 'DropOff_datetime': datetime.datetime(2024, 1, 1, 1, 5, 4), 'PULocationID': None, 'DOLocationID': 14.0, 'Affiliated_base_number': 'B00112', 'trip_miles': 0.0, 'driver_pay': 0.0, 'source': 'fhv_tripdata', 'trip_time': 477.0}, {'Pickup_datetime': datetime.datetime(2024, 1, 1, 0, 50, 18), 'DropOff_datetime': datetime.datetime(2024, 1, 1, 1, 29, 31), 'PULocationID': None, 'DOLocationID': 91.0, 'Affiliated_base_number': 'B00112', 'trip_miles': 0.0, 'driver_pay': 0.0, 'source': 'fhv_tripdata', 'trip_time': 2353.0}, {'Pickup_datetime': datetime.datetime(2024, 1, 1, 0, 11, 16), 'DropOff_datetime': datetime.datetime(2024, 1, 1, 0, 11, 21), 'PULocationID': None, 'DOLocationID': 14.0, 'Affiliated_base_number': 'B00112', 'trip_miles': 0.0, 'driver_pay': 0.0, 'source': 'fhv_tripdata', 'trip_time': 5.0}, {'Pickup_datetime': datetime.datetime(2024, 1, 1, 0, 38, 29), 'DropOff_datetime': datetime.datetime(2024, 1, 1, 0, 47, 47), 'PULocationID': None, 'DOLocationID': 14.0, 'Affiliated_base_number': 'B00112', 'trip_miles': 0.0, 'driver_pay': 0.0, 'source': 'fhv_tripdata', 'trip_time': 558.0}  ... displaying 10 of 10000 total bound parameter sets ...  {'Pickup_datetime': datetime.datetime(2024, 1, 1, 10, 32, 27), 'DropOff_datetime': datetime.datetime(2024, 1, 1, 10, 46, 53), 'PULocationID': None, 'DOLocationID': 42.0, 'Affiliated_base_number': 'B03404', 'trip_miles': 0.0, 'driver_pay': 0.0, 'source': 'fhv_tripdata', 'trip_time': 866.0}, {'Pickup_datetime': datetime.datetime(2024, 1, 1, 10, 23, 7), 'DropOff_datetime': datetime.datetime(2024, 1, 1, 10, 30, 55), 'PULocationID': None, 'DOLocationID': 3.0, 'Affiliated_base_number': 'B03404', 'trip_miles': 0.0, 'driver_pay': 0.0, 'source': 'fhv_tripdata', 'trip_time': 468.0}]]\n",
      "(Background on this error at: https://sqlalche.me/e/20/e3q8)\n",
      "Leyendo archivo D:\\GitHub\\Proyecto Final\\Proyecto_final_Henry\\Dataset\\green_tripdata_2024-08.parquet\n",
      "El archivo green_tripdata_2024-08.parquet contiene todas las columnas necesarias. Procesando...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando green_tripdata_2024-08.parquet:  17%|█▋        | 1/6 [00:04<00:21,  4.33s/chunk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/6 del archivo green_tripdata_2024-08.parquet cargado exitosamente.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando green_tripdata_2024-08.parquet:  33%|███▎      | 2/6 [00:08<00:17,  4.50s/chunk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 2/6 del archivo green_tripdata_2024-08.parquet cargado exitosamente.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando green_tripdata_2024-08.parquet:  50%|█████     | 3/6 [00:13<00:13,  4.62s/chunk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 3/6 del archivo green_tripdata_2024-08.parquet cargado exitosamente.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando green_tripdata_2024-08.parquet:  67%|██████▋   | 4/6 [00:18<00:09,  4.71s/chunk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 4/6 del archivo green_tripdata_2024-08.parquet cargado exitosamente.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando green_tripdata_2024-08.parquet:  83%|████████▎ | 5/6 [00:23<00:04,  4.71s/chunk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 5/6 del archivo green_tripdata_2024-08.parquet cargado exitosamente.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando green_tripdata_2024-08.parquet: 100%|██████████| 6/6 [00:25<00:00,  4.17s/chunk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 6/6 del archivo green_tripdata_2024-08.parquet cargado exitosamente.\n",
      "Datos del archivo green_tripdata_2024-08.parquet cargados exitosamente.\n",
      "Leyendo archivo D:\\GitHub\\Proyecto Final\\Proyecto_final_Henry\\Dataset\\yellow_tripdata_2024-01.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El archivo yellow_tripdata_2024-01.parquet contiene todas las columnas necesarias. Procesando...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando yellow_tripdata_2024-01.parquet:   0%|          | 1/297 [00:05<27:43,  5.62s/chunk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1/297 del archivo yellow_tripdata_2024-01.parquet cargado exitosamente.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando yellow_tripdata_2024-01.parquet:   1%|          | 2/297 [00:11<27:45,  5.64s/chunk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 2/297 del archivo yellow_tripdata_2024-01.parquet cargado exitosamente.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando yellow_tripdata_2024-01.parquet:   1%|          | 3/297 [00:16<27:00,  5.51s/chunk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 3/297 del archivo yellow_tripdata_2024-01.parquet cargado exitosamente.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando yellow_tripdata_2024-01.parquet:   1%|▏         | 4/297 [00:21<26:34,  5.44s/chunk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 4/297 del archivo yellow_tripdata_2024-01.parquet cargado exitosamente.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando yellow_tripdata_2024-01.parquet:   2%|▏         | 5/297 [00:26<25:26,  5.23s/chunk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 5/297 del archivo yellow_tripdata_2024-01.parquet cargado exitosamente.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando yellow_tripdata_2024-01.parquet:   2%|▏         | 6/297 [00:32<25:33,  5.27s/chunk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 6/297 del archivo yellow_tripdata_2024-01.parquet cargado exitosamente.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando yellow_tripdata_2024-01.parquet:   2%|▏         | 7/297 [00:36<24:21,  5.04s/chunk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 7/297 del archivo yellow_tripdata_2024-01.parquet cargado exitosamente.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando yellow_tripdata_2024-01.parquet:   3%|▎         | 8/297 [00:40<22:48,  4.73s/chunk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 8/297 del archivo yellow_tripdata_2024-01.parquet cargado exitosamente.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando yellow_tripdata_2024-01.parquet:   3%|▎         | 9/297 [00:45<22:50,  4.76s/chunk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 9/297 del archivo yellow_tripdata_2024-01.parquet cargado exitosamente.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando yellow_tripdata_2024-01.parquet:   3%|▎         | 10/297 [00:50<22:42,  4.75s/chunk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 10/297 del archivo yellow_tripdata_2024-01.parquet cargado exitosamente.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando yellow_tripdata_2024-01.parquet:   4%|▎         | 11/297 [00:55<23:03,  4.84s/chunk]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 11/297 del archivo yellow_tripdata_2024-01.parquet cargado exitosamente.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sqlalchemy\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Cargar las variables de entorno desde el archivo .env\n",
    "load_dotenv()\n",
    "\n",
    "# Obtener las credenciales de la base de datos desde las variables de entorno\n",
    "db_user = os.getenv('DB_USER')\n",
    "db_password = os.getenv('DB_PASSWORD')\n",
    "db_host = os.getenv('DB_HOST')\n",
    "db_port = os.getenv('DB_PORT')\n",
    "db_name = os.getenv('DB_NAME')\n",
    "\n",
    "# Conectar a MySQL utilizando las credenciales del archivo .env\n",
    "try:\n",
    "    engine = sqlalchemy.create_engine(f'mysql+pymysql://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}')\n",
    "    connection = engine.connect()\n",
    "    print(\"Conexión a la base de datos establecida con éxito.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error al conectar a la base de datos: {e}\")\n",
    "    raise\n",
    "\n",
    "# Ruta de la carpeta con los archivos\n",
    "folder_path = 'D:\\\\GitHub\\\\Proyecto Final\\\\Proyecto_final_Henry\\\\Dataset'\n",
    "\n",
    "# Columnas a mantener para green_tripdata\n",
    "columns_to_keep_green = ['lpep_pickup_datetime', 'lpep_dropoff_datetime', 'PULocationID', 'DOLocationID', 'trip_distance', 'total_amount']\n",
    "# Columnas a mantener para yellow_tripdata\n",
    "columns_to_keep_yellow = ['tpep_pickup_datetime', 'tpep_dropoff_datetime', 'PULocationID', 'DOLocationID', 'trip_distance', 'total_amount']\n",
    "# Columnas a mantener para fhv_tripdata\n",
    "columns_to_keep_fhv = ['pickup_datetime', 'dropOff_datetime', 'PUlocationID', 'DOlocationID', 'Affiliated_base_number']\n",
    "\n",
    "# Leer todos los archivos en la carpeta y estandarizar las columnas\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.parquet'):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        \n",
    "        try:\n",
    "            # Leer archivo .parquet\n",
    "            print(f\"Leyendo archivo {file_path}\")\n",
    "            df = pd.read_parquet(file_path)\n",
    "            \n",
    "            # Verificar si es green_tripdata, yellow_tripdata o fhv_tripdata\n",
    "            if 'green' in filename.lower():\n",
    "                columns_to_keep = columns_to_keep_green\n",
    "                source_value = 'green_tripdata'\n",
    "            elif 'yellow' in filename.lower():\n",
    "                columns_to_keep = columns_to_keep_yellow\n",
    "                source_value = 'yellow_tripdata'\n",
    "            elif 'fhv' in filename.lower():\n",
    "                columns_to_keep = columns_to_keep_fhv\n",
    "                source_value = 'fhv_tripdata'\n",
    "            else:\n",
    "                print(f\"El archivo {filename} no es ni 'green', 'yellow' ni 'fhv'. Saltando.\")\n",
    "                continue\n",
    "            \n",
    "            # Verificar si el archivo contiene todas las columnas necesarias\n",
    "            if all(column in df.columns for column in columns_to_keep):\n",
    "                print(f\"El archivo {filename} contiene todas las columnas necesarias. Procesando...\")\n",
    "                \n",
    "                # Seleccionar columnas necesarias\n",
    "                df = df[columns_to_keep]\n",
    "                \n",
    "                # Renombrar columnas según sea necesario\n",
    "                if 'yellow' in filename.lower():\n",
    "                    df.rename(columns={\n",
    "                        'tpep_pickup_datetime': 'Pickup_datetime',\n",
    "                        'tpep_dropoff_datetime': 'DropOff_datetime',\n",
    "                        'trip_distance': 'trip_miles',\n",
    "                        'total_amount': 'driver_pay'\n",
    "                    }, inplace=True)\n",
    "                elif 'green' in filename.lower():\n",
    "                    df.rename(columns={\n",
    "                        'lpep_pickup_datetime': 'Pickup_datetime',\n",
    "                        'lpep_dropoff_datetime': 'DropOff_datetime',\n",
    "                        'trip_distance': 'trip_miles',\n",
    "                        'total_amount': 'driver_pay'\n",
    "                    }, inplace=True)\n",
    "                elif 'fhv' in filename.lower():\n",
    "                    df.rename(columns={\n",
    "                        'pickup_datetime': 'Pickup_datetime',\n",
    "                        'dropOff_datetime': 'DropOff_datetime',\n",
    "                        'PUlocationID': 'PULocationID',\n",
    "                        'DOlocationID': 'DOLocationID'\n",
    "                    }, inplace=True)\n",
    "                    df['trip_miles'] = 0.0  # Agregar columna trip_miles con valores por defecto\n",
    "                    df['driver_pay'] = 0.0  # Agregar columna driver_pay con valores por defecto\n",
    "                \n",
    "                # Añadir la columna 'source'\n",
    "                df['source'] = source_value\n",
    "                \n",
    "                # Calcular trip_time como la diferencia entre DropOff_datetime y Pickup_datetime\n",
    "                df['Pickup_datetime'] = pd.to_datetime(df['Pickup_datetime'])\n",
    "                df['DropOff_datetime'] = pd.to_datetime(df['DropOff_datetime'])\n",
    "                df['trip_time'] = (df['DropOff_datetime'] - df['Pickup_datetime']).dt.total_seconds()\n",
    "                \n",
    "                # Dividir el DataFrame en bloques de 10,000 filas\n",
    "                num_chunks = len(df) // 10000 + 1\n",
    "                \n",
    "                for i in tqdm(range(num_chunks), desc=f\"Procesando {filename}\", unit=\"chunk\"):\n",
    "                    chunk = df[i * 10000:(i + 1) * 10000]\n",
    "                    chunk.to_sql(name='taxi_fhv_data', con=engine, if_exists='append', index=False)\n",
    "                    print(f\"Chunk {i+1}/{num_chunks} del archivo {filename} cargado exitosamente.\")\n",
    "                \n",
    "                print(f\"Datos del archivo {filename} cargados exitosamente.\")\n",
    "            else:\n",
    "                print(f\"El archivo {filename} no contiene todas las columnas necesarias. Saltando.\")\n",
    "        \n",
    "        except FileNotFoundError:\n",
    "            print(f\"Archivo {file_path} no encontrado. Saltando.\")\n",
    "        except KeyError as e:\n",
    "            print(f\"Error de columna en {file_path}: {e}. Saltando.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error al procesar el archivo {filename}: {e}\")\n",
    "\n",
    "# Ahora, inspecciona el archivo específico fhv_tripdata_2024-01.parquet\n",
    "file_to_inspect = 'fhv_tripdata_2024-01.parquet'\n",
    "file_path = os.path.join(folder_path, file_to_inspect)\n",
    "\n",
    "try:\n",
    "    # Leer archivo .parquet\n",
    "    df = pd.read_parquet(file_path)\n",
    "    \n",
    "    # Mostrar las columnas del archivo\n",
    "    print(f\"Columnas del archivo {file_to_inspect}:\")\n",
    "    print(df.columns.tolist())\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error al leer {file_path}: {e}\")\n",
    "\n",
    "print(\"Proceso de estandarización y carga completo.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
